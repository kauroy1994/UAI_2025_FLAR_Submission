{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Package Installations**"
      ],
      "metadata": {
        "id": "iEVJSyAbgnpA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cdt\n",
        "!pip install torch\n",
        "!pip install networkx\n",
        "!pip install causal-learn\n",
        "!pip install statsmodels\n",
        "!pip install xges\n",
        "!pip install numba"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2TPf8mdgr6X",
        "outputId": "37267b85-48b5-471b-957e-49f63dd52be6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting cdt\n",
            "  Downloading cdt-0.6.0-py3-none-any.whl.metadata (693 bytes)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from cdt) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from cdt) (1.14.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from cdt) (1.6.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from cdt) (1.4.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from cdt) (2.2.2)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.11/dist-packages (from cdt) (0.14.4)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from cdt) (3.4.2)\n",
            "Collecting skrebate (from cdt)\n",
            "  Downloading skrebate-0.62.tar.gz (19 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from cdt) (4.67.1)\n",
            "Collecting GPUtil (from cdt)\n",
            "  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from cdt) (2.32.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->cdt) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->cdt) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->cdt) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->cdt) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->cdt) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->cdt) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->cdt) (2025.1.31)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->cdt) (3.6.0)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.11/dist-packages (from statsmodels->cdt) (1.0.1)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from statsmodels->cdt) (24.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->cdt) (1.17.0)\n",
            "Downloading cdt-0.6.0-py3-none-any.whl (921 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m921.1/921.1 kB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: GPUtil, skrebate\n",
            "  Building wheel for GPUtil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for GPUtil: filename=GPUtil-1.4.0-py3-none-any.whl size=7392 sha256=e945313431f2c6615bfd3cf4831bf130e23b10456aa9d796ebd5f38d0f8c96ce\n",
            "  Stored in directory: /root/.cache/pip/wheels/2b/4d/8f/55fb4f7b9b591891e8d3f72977c4ec6c7763b39c19f0861595\n",
            "  Building wheel for skrebate (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for skrebate: filename=skrebate-0.62-py3-none-any.whl size=29253 sha256=8c59fd435f831cd28f5193dae84698de19b037cfa52672f3ff4b70934e90ad6a\n",
            "  Stored in directory: /root/.cache/pip/wheels/8a/d2/67/8368fca718041057b33d6448ca526aaf1ce2e404c94b901e41\n",
            "Successfully built GPUtil skrebate\n",
            "Installing collected packages: GPUtil, skrebate, cdt\n",
            "Successfully installed GPUtil-1.4.0 cdt-0.6.0 skrebate-0.62\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m105.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m86.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m89.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (3.4.2)\n",
            "Collecting causal-learn\n",
            "  Downloading causal_learn-0.1.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from causal-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from causal-learn) (1.14.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from causal-learn) (1.6.1)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from causal-learn) (0.20.3)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.11/dist-packages (from causal-learn) (0.14.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from causal-learn) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from causal-learn) (3.10.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from causal-learn) (3.4.2)\n",
            "Requirement already satisfied: pydot in /usr/local/lib/python3.11/dist-packages (from causal-learn) (3.0.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from causal-learn) (4.67.1)\n",
            "Collecting momentchi2 (from causal-learn)\n",
            "  Downloading momentchi2-0.1.8-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->causal-learn) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->causal-learn) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->causal-learn) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->causal-learn) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->causal-learn) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->causal-learn) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->causal-learn) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->causal-learn) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->causal-learn) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->causal-learn) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->causal-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->causal-learn) (3.6.0)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.11/dist-packages (from statsmodels->causal-learn) (1.0.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->causal-learn) (1.17.0)\n",
            "Downloading causal_learn-0.1.4.1-py3-none-any.whl (192 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m192.6/192.6 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading momentchi2-0.1.8-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: momentchi2, causal-learn\n",
            "Successfully installed causal-learn-0.1.4.1 momentchi2-0.1.8\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.11/dist-packages (0.14.4)\n",
            "Requirement already satisfied: numpy<3,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from statsmodels) (2.0.2)\n",
            "Requirement already satisfied: scipy!=1.9.2,>=1.8 in /usr/local/lib/python3.11/dist-packages (from statsmodels) (1.14.1)\n",
            "Requirement already satisfied: pandas!=2.1.0,>=1.4 in /usr/local/lib/python3.11/dist-packages (from statsmodels) (2.2.2)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.11/dist-packages (from statsmodels) (1.0.1)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from statsmodels) (24.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas!=2.1.0,>=1.4->statsmodels) (1.17.0)\n",
            "Collecting xges\n",
            "  Downloading xges-0.1.6-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting numba<0.60.0,>=0.59.1 (from xges)\n",
            "  Downloading numba-0.59.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.7 kB)\n",
            "Collecting numpy<2.0.0,>=1.26.4 (from xges)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sortedcontainers<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from xges) (2.4.0)\n",
            "Collecting llvmlite<0.43,>=0.42.0dev0 (from numba<0.60.0,>=0.59.1->xges)\n",
            "  Downloading llvmlite-0.42.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.8 kB)\n",
            "Downloading xges-0.1.6-py3-none-any.whl (26 kB)\n",
            "Downloading numba-0.59.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m73.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llvmlite-0.42.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (43.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 MB\u001b[0m \u001b[31m47.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, llvmlite, numba, xges\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: llvmlite\n",
            "    Found existing installation: llvmlite 0.43.0\n",
            "    Uninstalling llvmlite-0.43.0:\n",
            "      Successfully uninstalled llvmlite-0.43.0\n",
            "  Attempting uninstall: numba\n",
            "    Found existing installation: numba 0.60.0\n",
            "    Uninstalling numba-0.60.0:\n",
            "      Successfully uninstalled numba-0.60.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed llvmlite-0.42.0 numba-0.59.1 numpy-1.26.4 xges-0.1.6\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (0.59.1)\n",
            "Requirement already satisfied: llvmlite<0.43,>=0.42.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba) (0.42.0)\n",
            "Requirement already satisfied: numpy<1.27,>=1.22 in /usr/local/lib/python3.11/dist-packages (from numba) (1.26.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Environment Setup**"
      ],
      "metadata": {
        "id": "EaM912InmMZ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def set_seed(seed: int):\n",
        "    # Set seed for Python's built-in random module\n",
        "    random.seed(seed)\n",
        "\n",
        "    # Set seed for NumPy\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # Set seed for PyTorch on CPU\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    # Set seed for PyTorch on GPU (if using CUDA)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)  # If using multi-GPU\n",
        "\n",
        "    # Ensure deterministic behavior in PyTorch\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "    # Set PYTHONHASHSEED environment variable for reproducibility\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "set_seed(42)"
      ],
      "metadata": {
        "id": "mcc8Ip7hmOlh"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Helper Functions**"
      ],
      "metadata": {
        "id": "qSzYhUuF9XKo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "\"\"\"\n",
        "Complete Script to Compare PC and FLAR\n",
        "across metrics (SID, SHD, ATE_RMSE, runtime) for variable counts 10, 50, and 100.\n",
        "Evaluations are done for:\n",
        "  • DAG types: Erdős–Rényi (ER) and Scale‑Free (SF)\n",
        "  • SEM types: linear and non‑linear\n",
        "  • Noise types: gaussian, exponential, and laplace\n",
        "\n",
        "Make sure to install dependencies:\n",
        "    pip install numpy pandas torch networkx causal-learn statsmodels\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import networkx as nx\n",
        "import time\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 1. DAG Generation and Data Simulation Functions\n",
        "# ------------------------------------------------\n",
        "\n",
        "def generate_erdos_renyi_dag(num_nodes, edge_prob):\n",
        "    \"\"\"\n",
        "    Generate an Erdős–Rényi random DAG with given edge probability.\n",
        "    \"\"\"\n",
        "    perm = np.random.permutation(num_nodes)\n",
        "    adjacency_matrix = np.zeros((num_nodes, num_nodes), dtype=int)\n",
        "    for i in range(num_nodes):\n",
        "        for j in range(i+1, num_nodes):\n",
        "            if np.random.rand() < edge_prob:\n",
        "                adjacency_matrix[perm[i], perm[j]] = 1\n",
        "    return adjacency_matrix\n",
        "\n",
        "def remove_all_cycles(G):\n",
        "    \"\"\"\n",
        "    Remove edges from cycles until the graph is a DAG.\n",
        "    Works in-place on a networkx DiGraph.\n",
        "    \"\"\"\n",
        "    while True:\n",
        "        try:\n",
        "            cycle_edges = nx.find_cycle(G, orientation='original')\n",
        "            for (u, v, _) in cycle_edges:\n",
        "                G.remove_edge(u, v)\n",
        "        except nx.NetworkXNoCycle:\n",
        "            break\n",
        "    return G\n",
        "\n",
        "def generate_scale_free_dag(num_nodes):\n",
        "    \"\"\"\n",
        "    Generate a scale-free DAG: first generate a scale-free network and then remove cycles.\n",
        "    \"\"\"\n",
        "    G = nx.scale_free_graph(num_nodes, seed=None)\n",
        "    G_simple = nx.DiGraph(G)  # convert to simple directed graph\n",
        "    G_simple.remove_edges_from(nx.selfloop_edges(G_simple))\n",
        "    G_dag = remove_all_cycles(G_simple.copy())\n",
        "    adj_matrix = nx.to_numpy_array(G_dag, dtype=int)\n",
        "    np.fill_diagonal(adj_matrix, 0)\n",
        "    return adj_matrix\n",
        "\n",
        "def simulate_sem_with_weights(adjacency_matrix, n_samples, noise_type='gaussian',\n",
        "                              weight_scale=1.0, random_state=None):\n",
        "    \"\"\"\n",
        "    Simulate data from a linear SEM:\n",
        "         X_j = sum_{i in Pa(j)} W_{i,j} * X_i + noise_j\n",
        "    Returns data X and the true weight matrix W.\n",
        "    \"\"\"\n",
        "    if random_state is not None:\n",
        "        np.random.seed(random_state)\n",
        "\n",
        "    num_nodes = adjacency_matrix.shape[0]\n",
        "    G = nx.DiGraph(adjacency_matrix)\n",
        "    topo_order = list(nx.topological_sort(G))\n",
        "\n",
        "    # Generate weights on edges\n",
        "    W = np.zeros((num_nodes, num_nodes))\n",
        "    for i in range(num_nodes):\n",
        "        for j in range(num_nodes):\n",
        "            if adjacency_matrix[i, j] == 1:\n",
        "                W[i, j] = np.random.normal(loc=0.0, scale=weight_scale)\n",
        "\n",
        "    X = np.zeros((n_samples, num_nodes))\n",
        "    for s in range(n_samples):\n",
        "        for node in topo_order:\n",
        "            parents = np.where(adjacency_matrix[:, node] == 1)[0]\n",
        "            parents_sum = np.sum(W[parents, node] * X[s, parents])\n",
        "            if noise_type.lower() == 'gaussian':\n",
        "                noise = np.random.normal(loc=0.0, scale=1.0)\n",
        "            elif noise_type.lower() == 'laplace':\n",
        "                noise = np.random.laplace(loc=0.0, scale=1.0)\n",
        "            elif noise_type.lower() == 'exponential':\n",
        "                noise = np.random.exponential(scale=1.0)\n",
        "            else:\n",
        "                raise ValueError(\"noise_type must be 'gaussian', 'laplace', or 'exponential'.\")\n",
        "            X[s, node] = parents_sum + noise\n",
        "    return X, W\n",
        "\n",
        "def simulate_non_linear_sem_with_weights(adjacency_matrix, n_samples, noise_type='gaussian',\n",
        "                                           weight_scale=1.0, random_state=None, non_linear_fn=np.tanh):\n",
        "    \"\"\"\n",
        "    Simulate data from a non-linear SEM:\n",
        "         X_j = f( sum_{i in Pa(j)} W_{i,j} * X_i ) + noise_j\n",
        "    Returns data X and the true weight matrix W.\n",
        "    \"\"\"\n",
        "    if random_state is not None:\n",
        "        np.random.seed(random_state)\n",
        "\n",
        "    num_nodes = adjacency_matrix.shape[0]\n",
        "    G = nx.DiGraph(adjacency_matrix)\n",
        "    topo_order = list(nx.topological_sort(G))\n",
        "\n",
        "    # Generate weights on edges\n",
        "    W = np.zeros((num_nodes, num_nodes))\n",
        "    for i in range(num_nodes):\n",
        "        for j in range(num_nodes):\n",
        "            if adjacency_matrix[i, j] == 1:\n",
        "                W[i, j] = np.random.normal(loc=0.0, scale=weight_scale)\n",
        "\n",
        "    X = np.zeros((n_samples, num_nodes))\n",
        "    for s in range(n_samples):\n",
        "        for node in topo_order:\n",
        "            parents = np.where(adjacency_matrix[:, node] == 1)[0]\n",
        "            parents_sum = np.sum(W[parents, node] * X[s, parents])\n",
        "            non_linear_term = non_linear_fn(parents_sum)\n",
        "            if noise_type.lower() == 'gaussian':\n",
        "                noise = np.random.normal(loc=0.0, scale=1.0)\n",
        "            elif noise_type.lower() == 'laplace':\n",
        "                noise = np.random.laplace(loc=0.0, scale=1.0)\n",
        "            elif noise_type.lower() == 'exponential':\n",
        "                noise = np.random.exponential(scale=1.0)\n",
        "            else:\n",
        "                raise ValueError(\"noise_type must be 'gaussian', 'laplace', or 'exponential'.\")\n",
        "            X[s, node] = non_linear_term + noise\n",
        "    return X, W\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 2. Metrics: SHD, SID, and ATE_RMSE\n",
        "# ------------------------------------------------\n",
        "\n",
        "def shd(true_adj: np.ndarray, est_adj: np.ndarray) -> int:\n",
        "    \"\"\"Compute Structural Hamming Distance.\"\"\"\n",
        "    return int(np.sum(true_adj != est_adj))\n",
        "\n",
        "def _compute_ancestors(adj: np.ndarray):\n",
        "    \"\"\"Helper for SID calculation.\"\"\"\n",
        "    G = nx.DiGraph(adj)\n",
        "    d = adj.shape[0]\n",
        "    ancestors_list = []\n",
        "    for node in range(d):\n",
        "        ancestors_list.append(set(nx.ancestors(G, node)))\n",
        "    return ancestors_list\n",
        "\n",
        "def sid(true_adj: np.ndarray, est_adj: np.ndarray) -> int:\n",
        "    \"\"\"Compute Structural Intervention Distance (SID).\"\"\"\n",
        "    true_anc = _compute_ancestors(true_adj)\n",
        "    est_anc = _compute_ancestors(est_adj)\n",
        "    d = true_adj.shape[0]\n",
        "    score = 0\n",
        "    for j in range(d):\n",
        "        diff_1 = true_anc[j].difference(est_anc[j])\n",
        "        diff_2 = est_anc[j].difference(true_anc[j])\n",
        "        score += len(diff_1) + len(diff_2)\n",
        "    return score\n",
        "\n",
        "def compute_total_effect_matrix(W: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Compute total effect matrix T = (I - W)^{-1} - I.\"\"\"\n",
        "    d = W.shape[0]\n",
        "    I = np.eye(d)\n",
        "    try:\n",
        "        inv = np.linalg.inv(I - W)\n",
        "    except np.linalg.LinAlgError:\n",
        "        return np.zeros((d, d))\n",
        "    return inv - I\n",
        "\n",
        "def rmse_ate(W_true: np.ndarray, W_est: np.ndarray) -> float:\n",
        "    \"\"\"Compute RMSE of total causal effects (ATE RMSE).\"\"\"\n",
        "    T_true = compute_total_effect_matrix(W_true)\n",
        "    T_est  = compute_total_effect_matrix(W_est)\n",
        "    return np.sqrt(np.mean((T_true - T_est) ** 2))"
      ],
      "metadata": {
        "id": "iZyhh-pC9ZPF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **FLAR**"
      ],
      "metadata": {
        "id": "DqLfPFUM_14Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------\n",
        "# 3. FLAR Implementation\n",
        "# ------------------------------------------------\n",
        "\n",
        "def squared_loss(x_true, x_pred):\n",
        "    return 0.5 * torch.mean((x_true - x_pred)**2)\n",
        "\n",
        "def dag_constraint(W):\n",
        "    d = W.shape[0]\n",
        "    WW = W * W\n",
        "    expm_WW = torch.matrix_exp(WW)\n",
        "    h = torch.trace(expm_WW) - d\n",
        "    return h\n",
        "\n",
        "def apply_mask(W, mask):\n",
        "    with torch.no_grad():\n",
        "        W *= mask\n",
        "        d = W.shape[0]\n",
        "        for i in range(d):\n",
        "            W[i, i] = 0.0\n",
        "\n",
        "def binarize_adjacency(W, threshold=0.3):\n",
        "    W_np = W.detach().cpu().numpy()\n",
        "    W_bin = (np.abs(W_np) > threshold).astype(float)\n",
        "    np.fill_diagonal(W_bin, 0.0)\n",
        "    return W_bin\n",
        "\n",
        "class WeakLearnerNN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=40):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class FunctionalBoostingModel(nn.Module):\n",
        "    def __init__(self, d, max_num_weak_learners=50, hidden_dim=40):\n",
        "        super().__init__()\n",
        "        self.d = d\n",
        "        self.max_num_weak_learners = max_num_weak_learners\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.learners_for_var = [[] for _ in range(d)]\n",
        "        self.weak_learners = nn.ModuleList()\n",
        "        self.current_counts = [0]*d\n",
        "\n",
        "    def forward(self, X, W):\n",
        "        N, d = X.shape\n",
        "        device = X.device\n",
        "        Xhat = []\n",
        "        for i in range(d):\n",
        "            mask_row = W[i, :]\n",
        "            masked_input = X * mask_row\n",
        "            pred_i = torch.zeros((N, 1), dtype=X.dtype, device=device)\n",
        "            for learner in self.learners_for_var[i]:\n",
        "                pred_i += learner(masked_input)\n",
        "            Xhat.append(pred_i)\n",
        "        return torch.cat(Xhat, dim=1)\n",
        "\n",
        "    def add_weak_learner(self, i):\n",
        "        wl = WeakLearnerNN(input_dim=self.d, hidden_dim=self.hidden_dim)\n",
        "        self.weak_learners.append(wl)\n",
        "        self.learners_for_var[i].append(wl)\n",
        "        self.current_counts[i] += 1\n",
        "\n",
        "    def fit_new_weak_learner(self, i, X, residual_i, W, n_epochs=5, lr=0.01, verbose=False):\n",
        "        self.add_weak_learner(i)\n",
        "        wl = self.learners_for_var[i][-1]\n",
        "        optimizer = optim.Adam(wl.parameters(), lr=lr)\n",
        "        for epoch in range(n_epochs):\n",
        "            optimizer.zero_grad()\n",
        "            mask_row = W[i, :]\n",
        "            masked_input = X * mask_row\n",
        "            pred = wl(masked_input)\n",
        "            loss = torch.mean((pred - residual_i)**2)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            if verbose:\n",
        "                print(f\"    [WL-Fit Var {i} Ep {epoch}] Loss={loss.item():.6f}\")\n",
        "\n",
        "class DAGBoostingTrainer:\n",
        "    def __init__(\n",
        "        self,\n",
        "        d,\n",
        "        adjacency_mask=None,\n",
        "        lr_W=0.01,\n",
        "        lambda_h=5.0,\n",
        "        alpha_init=0.0,\n",
        "        max_iter=5,\n",
        "        max_num_weak_learners=50,\n",
        "        hidden_dim=40,\n",
        "        tol=1e-4,\n",
        "        patience=2,\n",
        "        device=torch.device(\"cpu\")\n",
        "    ):\n",
        "        self.d = d\n",
        "        self.model = FunctionalBoostingModel(\n",
        "            d=d,\n",
        "            max_num_weak_learners=max_num_weak_learners,\n",
        "            hidden_dim=hidden_dim\n",
        "        ).to(device)\n",
        "        # Initialize real-valued adjacency matrix W\n",
        "        W_init = 0.01 * torch.randn(d, d, device=device)\n",
        "        for i in range(d):\n",
        "            W_init[i, i] = 0.0\n",
        "        self.W = nn.Parameter(W_init)\n",
        "\n",
        "        if adjacency_mask is None:\n",
        "            adjacency_mask = np.ones((d, d), dtype=np.float32)\n",
        "            np.fill_diagonal(adjacency_mask, 0.)\n",
        "        self.adjacency_mask = torch.tensor(adjacency_mask, dtype=torch.float32, device=device)\n",
        "\n",
        "        self.lambda_h = lambda_h\n",
        "        self.alpha = alpha_init\n",
        "        self.lr_W = lr_W\n",
        "        self.max_iter = max_iter\n",
        "        self.tol = tol\n",
        "        self.patience = patience\n",
        "        self.device = device\n",
        "        self.best_loss = float('inf')\n",
        "        self.no_improv_steps = 0\n",
        "        self.stop_early = False\n",
        "\n",
        "    def parameters(self):\n",
        "        return list(self.model.parameters()) + [self.W]\n",
        "\n",
        "    def apply_domain_mask_and_no_loops(self):\n",
        "        apply_mask(self.W, self.adjacency_mask)\n",
        "\n",
        "    def augmented_lagrangian_loss(self, X):\n",
        "        Xhat = self.model(X, self.W)\n",
        "        recon = squared_loss(X, Xhat)\n",
        "        h_val = dag_constraint(self.W)\n",
        "        aug = self.alpha * h_val + 0.5 * self.lambda_h * (h_val ** 2)\n",
        "        return recon + aug, recon, h_val\n",
        "\n",
        "    def update_dual(self, h_val):\n",
        "        self.alpha = self.alpha + self.lambda_h * h_val.item()\n",
        "\n",
        "    def train(self, X, batch_size=512, n_inner_epochs=10, fit_new_learner_epochs=5, verbose=True):\n",
        "        if not isinstance(X, torch.Tensor):\n",
        "            X = torch.tensor(X, dtype=torch.float32, device=self.device)\n",
        "        else:\n",
        "            X = X.to(self.device)\n",
        "\n",
        "        N_full = X.shape[0]\n",
        "\n",
        "        for outer_iter in range(self.max_iter):\n",
        "            if verbose:\n",
        "                print(f\"\\n===== Outer Iteration {outer_iter+1}/{self.max_iter} =====\")\n",
        "            indices = np.random.permutation(N_full)\n",
        "            subset_idx = indices[:batch_size]\n",
        "            X_sub = X[subset_idx]\n",
        "\n",
        "            with torch.no_grad():\n",
        "                Xhat_sub = self.model(X_sub, self.W)\n",
        "                residuals_sub = X_sub - Xhat_sub\n",
        "                mse_val = torch.mean((residuals_sub)**2).item()\n",
        "            if verbose:\n",
        "                print(f\"  Sub-batch size={batch_size}, MSE before new learners: {mse_val:.6f}\")\n",
        "\n",
        "            d = X_sub.shape[1]\n",
        "            for i in range(d):\n",
        "                if self.model.current_counts[i] < self.model.max_num_weak_learners:\n",
        "                    residual_i_sub = residuals_sub[:, i:i+1]\n",
        "                    self.model.fit_new_weak_learner(i=i, X=X_sub, residual_i=residual_i_sub,\n",
        "                                                     W=self.W, n_epochs=fit_new_learner_epochs,\n",
        "                                                     lr=0.01, verbose=False)\n",
        "\n",
        "            # Update W via inner epochs\n",
        "            opt = optim.Adam(self.parameters(), lr=self.lr_W)\n",
        "            for epoch in range(n_inner_epochs):\n",
        "                opt.zero_grad()\n",
        "                loss_total, loss_recon, h_val = self.augmented_lagrangian_loss(X_sub)\n",
        "                loss_total.backward()\n",
        "                opt.step()\n",
        "                self.apply_domain_mask_and_no_loops()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                _, _, h_val = self.augmented_lagrangian_loss(X_sub)\n",
        "            self.update_dual(h_val)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                Xhat_sub = self.model(X_sub, self.W)\n",
        "                mse_val = torch.mean((X_sub - Xhat_sub)**2).item()\n",
        "                h_now = dag_constraint(self.W).item()\n",
        "            if verbose:\n",
        "                print(f\"  [Iteration {outer_iter+1}] MSE={mse_val:.6f}, h(W)={h_now:.6f}, alpha={self.alpha:.3f}\")\n",
        "            if mse_val < self.best_loss - self.tol:\n",
        "                self.best_loss = mse_val\n",
        "                self.no_improv_steps = 0\n",
        "            else:\n",
        "                self.no_improv_steps += 1\n",
        "            if self.no_improv_steps >= self.patience:\n",
        "                if verbose:\n",
        "                    print(\"No improvement; early stopping.\")\n",
        "                self.stop_early = True\n",
        "                break\n",
        "        return self.W.detach(), self.model\n",
        "\n",
        "    def get_binarized_adjacency(self, threshold=0.3):\n",
        "        return binarize_adjacency(self.W, threshold=threshold)\n",
        "\n",
        "def run_dagboost_method(X_data, threshold=0.3, max_iter=5, max_num_weak_learners=50,\n",
        "                        hidden_dim=40, lambda_h=5.0, verbose=False):\n",
        "    \"\"\"\n",
        "    Run the DAG‑Boosting (FLAR) method and return:\n",
        "       - final binary estimated adjacency matrix, and\n",
        "       - the trainer (for further access to model/W)\n",
        "    \"\"\"\n",
        "    device = torch.device(\"cpu\")\n",
        "    d = X_data.shape[1]\n",
        "    adjacency_mask = np.ones((d, d), dtype=np.float32)\n",
        "    np.fill_diagonal(adjacency_mask, 0.0)\n",
        "    trainer = DAGBoostingTrainer(\n",
        "        d=d,\n",
        "        adjacency_mask=adjacency_mask,\n",
        "        lr_W=0.01,\n",
        "        lambda_h=lambda_h,\n",
        "        alpha_init=0.0,\n",
        "        max_iter=max_iter,\n",
        "        max_num_weak_learners=max_num_weak_learners,\n",
        "        hidden_dim=hidden_dim,\n",
        "        tol=1e-5,\n",
        "        patience=2,\n",
        "        device=device\n",
        "    )\n",
        "    trainer.train(X_data, batch_size=128, n_inner_epochs=10, fit_new_learner_epochs=5, verbose=verbose)\n",
        "    W_bin = trainer.get_binarized_adjacency(threshold=threshold)\n",
        "    # Remove any residual cycles\n",
        "    G_nx = nx.DiGraph(W_bin)\n",
        "    remove_all_cycles(G_nx)\n",
        "    final_adj = nx.to_numpy_array(G_nx, dtype=int)\n",
        "    return final_adj, trainer\n",
        "\n",
        "def flar(X_data, threshold=0.3, max_iter=5, max_num_weak_learners=50, hidden_dim=40,\n",
        "         lambda_h=5.0, verbose=False):\n",
        "    \"\"\"\n",
        "    Wrapper for running DAG‑Boosting.\n",
        "    \"\"\"\n",
        "    return run_dagboost_method(X_data, threshold=threshold, max_iter=max_iter,\n",
        "                               max_num_weak_learners=max_num_weak_learners,\n",
        "                               hidden_dim=hidden_dim, lambda_h=lambda_h, verbose=verbose)"
      ],
      "metadata": {
        "id": "sB_6kZXl_3YU"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Comparison Against PC**"
      ],
      "metadata": {
        "id": "6DGNWu2ff5gW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 4. PC Method (using causal‑learn)\n",
        "# ------------------------------------------------\n",
        "\n",
        "def run_pc_method(X_data):\n",
        "    \"\"\"\n",
        "    Run the PC algorithm from causal‑learn on data X_data.\n",
        "    Based on the usage in the causal-learn docs, PC is invoked as:\n",
        "         from causallearn.search.ConstraintBased.PC import pc\n",
        "         result = pc(X_data, alpha=0.05)\n",
        "    This function converts the resulting graph into a binary adjacency matrix,\n",
        "    where an entry of 1 in est_adj[i,j] indicates an edge i -> j.\n",
        "    \"\"\"\n",
        "    from causallearn.search.ConstraintBased.PC import pc\n",
        "    try:\n",
        "        # Run PC with a default significance level (alpha=0.05)\n",
        "        pc_result = pc(X_data, alpha=0.05)\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"PC algorithm failed with error: {e}\")\n",
        "\n",
        "    # Convert the result to an adjacency matrix.\n",
        "    # Here we assume pc_result.G.graph is a NumPy array encoding the graph.\n",
        "    pc_matrix = pc_result.G.graph\n",
        "    d = pc_matrix.shape[0]\n",
        "    est_adj = np.zeros((d, d), dtype=int)\n",
        "    # For each pair, if pc_matrix[j,i]==1, then there is an edge i -> j.\n",
        "    for i in range(d):\n",
        "        for j in range(d):\n",
        "            if pc_matrix[j, i] == 1:\n",
        "                est_adj[i, j] = 1\n",
        "    # Remove cycles if any\n",
        "    G_nx = nx.DiGraph(est_adj)\n",
        "    remove_all_cycles(G_nx)\n",
        "    final_adj = nx.to_numpy_array(G_nx, dtype=int)\n",
        "    return final_adj\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 5. Main Comparison Loop: Testing Configurations\n",
        "# ------------------------------------------------\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    run_cell = True  # Set run_cell to True to execute\n",
        "    if not run_cell:\n",
        "        class StopExecutionWithMessage(Exception):\n",
        "            def __init__(self, message):\n",
        "                self.message = message\n",
        "            def _render_traceback_(self):\n",
        "                print(self.message)\n",
        "                return []\n",
        "        raise StopExecutionWithMessage(\"Execution halted.\")\n",
        "\n",
        "    # Experimental configurations\n",
        "    var_settings = [500]       # Number of variables\n",
        "    dag_types = ['ER']#,'SF']           # ER: Erdős–Rényi, SF: Scale-Free\n",
        "    sem_types = ['linear']#,'non-linear']\n",
        "    noise_types = ['gaussian']#,'exponential','laplace']  # Use \"laplace\" for Laplace noise\n",
        "    n_samples = 500                    # Number of samples per experiment\n",
        "    weight_scale = 1.0\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for n_vars in var_settings:\n",
        "        edge_prob = 2.0 / (n_vars - 1)  # Expected average degree ~2\n",
        "        for dag_type in dag_types:\n",
        "            # Generate the true DAG\n",
        "            if dag_type == 'ER':\n",
        "                true_adj = generate_erdos_renyi_dag(n_vars, edge_prob)\n",
        "            elif dag_type == 'SF':\n",
        "                true_adj = generate_scale_free_dag(n_vars)\n",
        "            else:\n",
        "                continue\n",
        "\n",
        "            for sem_type in sem_types:\n",
        "                for noise in noise_types:\n",
        "                    config_str = f\"n_vars={n_vars}, DAG={dag_type}, SEM={sem_type}, Noise={noise}\"\n",
        "                    print(\"\\n[Config] \" + config_str)\n",
        "                    # Simulate data and obtain true weight matrix\n",
        "                    if sem_type == 'linear':\n",
        "                        X_data, W_true = simulate_sem_with_weights(true_adj, n_samples,\n",
        "                                                                   noise_type=noise,\n",
        "                                                                   weight_scale=weight_scale)\n",
        "                    else:\n",
        "                        X_data, W_true = simulate_non_linear_sem_with_weights(true_adj, n_samples,\n",
        "                                                                              noise_type=noise,\n",
        "                                                                              weight_scale=weight_scale,\n",
        "                                                                              non_linear_fn=np.tanh)\n",
        "\n",
        "                    # Create normalized data (standardization: zero mean, unit std)\n",
        "                    X_data_norm = (X_data - X_data.mean(axis=0)) / (X_data.std(axis=0) + 1e-8)\n",
        "                    # ---------------------------\n",
        "                    # Run PC Method\n",
        "                    try:\n",
        "                        t0 = time.time()\n",
        "                        est_adj_pc = run_pc_method(X_data)\n",
        "                        runtime_pc = time.time() - t0\n",
        "                        shd_pc = shd(true_adj, est_adj_pc)\n",
        "                        sid_pc = sid(true_adj, est_adj_pc)\n",
        "                        ate_rmse_pc = rmse_ate(W_true, est_adj_pc.astype(float))\n",
        "                    except Exception as e:\n",
        "                        runtime_pc = np.nan\n",
        "                        shd_pc = np.nan\n",
        "                        sid_pc = np.nan\n",
        "                        ate_rmse_pc = np.nan\n",
        "                        print(\"  PC method failed with error:\", e)\n",
        "\n",
        "                    results.append({\n",
        "                        'Method': 'PC',\n",
        "                        'n_vars': n_vars,\n",
        "                        'DAG_type': dag_type,\n",
        "                        'SEM_type': sem_type,\n",
        "                        'Noise': noise,\n",
        "                        'Runtime_sec': runtime_pc,\n",
        "                        'SHD': shd_pc,\n",
        "                        'SID': sid_pc,\n",
        "                        'ATE_RMSE': ate_rmse_pc\n",
        "                    })\n",
        "\n",
        "                    # ---------------------------\n",
        "                    # Run DAG‑Boosting (FLAR) Method\n",
        "                    try:\n",
        "                        t0 = time.time()\n",
        "                        est_adj_dagboost, trainer = flar(X_data, threshold=0.3, verbose=False)\n",
        "                        runtime_dagboost = time.time() - t0\n",
        "                        shd_dagboost = shd(true_adj, est_adj_dagboost)\n",
        "                        sid_dagboost = sid(true_adj, est_adj_dagboost)\n",
        "                        ate_rmse_dagboost = rmse_ate(W_true, est_adj_dagboost.astype(float))\n",
        "                    except Exception as e:\n",
        "                        runtime_dagboost = np.nan\n",
        "                        shd_dagboost = np.nan\n",
        "                        sid_dagboost = np.nan\n",
        "                        ate_rmse_dagboost = np.nan\n",
        "                        print(\"  DAG‑Boosting method failed with error:\", e)\n",
        "\n",
        "                    results.append({\n",
        "                        'Method': 'DAGBoost',\n",
        "                        'n_vars': n_vars,\n",
        "                        'DAG_type': dag_type,\n",
        "                        'SEM_type': sem_type,\n",
        "                        'Noise': noise,\n",
        "                        'Runtime_sec': runtime_dagboost,\n",
        "                        'SHD': shd_dagboost,\n",
        "                        'SID': sid_dagboost,\n",
        "                        'ATE_RMSE': ate_rmse_dagboost\n",
        "                    })\n",
        "\n",
        "    # Compile and print results as a DataFrame\n",
        "    df_results = pd.DataFrame(results)\n",
        "    print(\"\\n===== Comparison Results =====\")\n",
        "    print(df_results)\n",
        "\n",
        "    # Save the results DataFrame as CSV and JSON files\n",
        "    output_csv_filename = \"PC_results_500.csv\"\n",
        "    output_json_filename = \"PC_results_500.json\"\n",
        "\n",
        "    # Save to CSV (with headers, no index)\n",
        "    df_results.to_csv(output_csv_filename, index=False)\n",
        "    print(f\"Results saved to {output_csv_filename}\")\n",
        "\n",
        "    # Save to JSON in records format (one JSON object per line)\n",
        "    df_results.to_json(output_json_filename, orient=\"records\", lines=True)\n",
        "    print(f\"Results saved to {output_json_filename}\")"
      ],
      "metadata": {
        "id": "1IqD0DRigFGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Comparison with GES**"
      ],
      "metadata": {
        "id": "_6NJFfJzl2gj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_ges_method(X_data, score_func='local_score_BIC', maxP=None, parameters=None):\n",
        "    \"\"\"\n",
        "    Run the GES algorithm from causal‑learn on data X_data.\n",
        "    By default uses the BIC score.\n",
        "\n",
        "    Returns a binary adjacency matrix such that est_adj[i,j]=1 indicates edge i → j.\n",
        "    \"\"\"\n",
        "    from causallearn.search.ScoreBased.GES import ges\n",
        "    try:\n",
        "        Record = ges(X_data, score_func, maxP, parameters)\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"GES algorithm failed with error: {e}\")\n",
        "\n",
        "    ges_matrix = Record['G'].graph  # Expecting a NumPy array encoding the graph.\n",
        "    d = ges_matrix.shape[0]\n",
        "    est_adj = np.zeros((d, d), dtype=int)\n",
        "    # For each pair, if ges_matrix[j,i]==1 then an edge i → j is inferred.\n",
        "    for i in range(d):\n",
        "        for j in range(d):\n",
        "            if ges_matrix[j, i] == 1:\n",
        "                est_adj[i, j] = 1\n",
        "    # Remove any residual cycles.\n",
        "    G_nx = nx.DiGraph(est_adj)\n",
        "    remove_all_cycles(G_nx)\n",
        "    final_adj = nx.to_numpy_array(G_nx, dtype=int)\n",
        "    return final_adj\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    run_cell = False  # Set run_cell to True to execute\n",
        "    if not run_cell:\n",
        "        class StopExecutionWithMessage(Exception):\n",
        "            def __init__(self, message):\n",
        "                self.message = message\n",
        "            def _render_traceback_(self):\n",
        "                print(self.message)\n",
        "                return []\n",
        "        raise StopExecutionWithMessage(\"Execution halted.\")\n",
        "\n",
        "    # Experimental configurations\n",
        "    var_settings = [10, 20]       # Number of variables\n",
        "    dag_types = ['ER', 'SF']           # ER: Erdős–Rényi, SF: Scale‑Free\n",
        "    sem_types = ['linear', 'non-linear']\n",
        "    noise_types = ['gaussian', 'exponential', 'laplace']  # Use \"laplace\" for Laplace noise\n",
        "    n_samples = 500                    # Number of samples per experiment\n",
        "    weight_scale = 1.0\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for n_vars in var_settings:\n",
        "        # Set edge probability for an expected average degree of ~2\n",
        "        edge_prob = 2.0 / (n_vars - 1)\n",
        "        for dag_type in dag_types:\n",
        "            # Generate the true DAG\n",
        "            if dag_type == 'ER':\n",
        "                true_adj = generate_erdos_renyi_dag(n_vars, edge_prob)\n",
        "            elif dag_type == 'SF':\n",
        "                true_adj = generate_scale_free_dag(n_vars)\n",
        "            else:\n",
        "                continue\n",
        "\n",
        "            for sem_type in sem_types:\n",
        "                for noise in noise_types:\n",
        "                    config_str = f\"n_vars={n_vars}, DAG={dag_type}, SEM={sem_type}, Noise={noise}\"\n",
        "                    print(\"\\n[Config] \" + config_str)\n",
        "                    # Simulate data and obtain true weight matrix\n",
        "                    if sem_type == 'linear':\n",
        "                        X_data, W_true = simulate_sem_with_weights(true_adj, n_samples,\n",
        "                                                                   noise_type=noise,\n",
        "                                                                   weight_scale=weight_scale)\n",
        "                    else:  # non-linear\n",
        "                        X_data, W_true = simulate_non_linear_sem_with_weights(true_adj, n_samples,\n",
        "                                                                              noise_type=noise,\n",
        "                                                                              weight_scale=weight_scale,\n",
        "                                                                              non_linear_fn=np.tanh)\n",
        "                    # Create normalized data (standardization: zero mean, unit std)\n",
        "                    X_data_norm = (X_data - X_data.mean(axis=0)) / (X_data.std(axis=0) + 1e-8)\n",
        "                    # ---------------------------\n",
        "                    # Run GES Method\n",
        "                    try:\n",
        "                        t0 = time.time()\n",
        "                        est_adj_ges = run_ges_method(X_data)\n",
        "                        runtime_ges = time.time() - t0\n",
        "                        shd_ges = shd(true_adj, est_adj_ges)\n",
        "                        sid_ges = sid(true_adj, est_adj_ges)\n",
        "                        ate_rmse_ges = rmse_ate(W_true, est_adj_ges.astype(float))\n",
        "                    except Exception as e:\n",
        "                        runtime_ges = np.nan\n",
        "                        shd_ges = np.nan\n",
        "                        sid_ges = np.nan\n",
        "                        ate_rmse_ges = np.nan\n",
        "                        print(\"  GES method failed with error:\", e)\n",
        "\n",
        "                    results.append({\n",
        "                        'Method': 'GES',\n",
        "                        'n_vars': n_vars,\n",
        "                        'DAG_type': dag_type,\n",
        "                        'SEM_type': sem_type,\n",
        "                        'Noise': noise,\n",
        "                        'Runtime_sec': runtime_ges,\n",
        "                        'SHD': shd_ges,\n",
        "                        'SID': sid_ges,\n",
        "                        'ATE_RMSE': ate_rmse_ges\n",
        "                    })\n",
        "\n",
        "                    # ---------------------------\n",
        "                    # Run DAG‑Boosting (FLAR) Method\n",
        "                    try:\n",
        "                        t0 = time.time()\n",
        "                        est_adj_dagboost, trainer = flar(X_data, threshold=0.3, verbose=False)\n",
        "                        runtime_dagboost = time.time() - t0\n",
        "                        shd_dagboost = shd(true_adj, est_adj_dagboost)\n",
        "                        sid_dagboost = sid(true_adj, est_adj_dagboost)\n",
        "                        ate_rmse_dagboost = rmse_ate(W_true, est_adj_dagboost.astype(float))\n",
        "                    except Exception as e:\n",
        "                        runtime_dagboost = np.nan\n",
        "                        shd_dagboost = np.nan\n",
        "                        sid_dagboost = np.nan\n",
        "                        ate_rmse_dagboost = np.nan\n",
        "                        print(\"  DAG‑Boosting method failed with error:\", e)\n",
        "\n",
        "                    results.append({\n",
        "                        'Method': 'DAGBoost',\n",
        "                        'n_vars': n_vars,\n",
        "                        'DAG_type': dag_type,\n",
        "                        'SEM_type': sem_type,\n",
        "                        'Noise': noise,\n",
        "                        'Runtime_sec': runtime_dagboost,\n",
        "                        'SHD': shd_dagboost,\n",
        "                        'SID': sid_dagboost,\n",
        "                        'ATE_RMSE': ate_rmse_dagboost\n",
        "                    })\n",
        "\n",
        "    # Compile and print results as a DataFrame\n",
        "    df_results = pd.DataFrame(results)\n",
        "    print(\"\\n===== Comparison Results =====\")\n",
        "    print(df_results)\n",
        "\n",
        "    # Save the results DataFrame as CSV and JSON files\n",
        "    output_csv_filename = \"GES_results.csv\"\n",
        "    output_json_filename = \"GES_results.json\"\n",
        "\n",
        "    # Save to CSV (with headers, no index)\n",
        "    df_results.to_csv(output_csv_filename, index=False)\n",
        "    print(f\"Results saved to {output_csv_filename}\")\n",
        "\n",
        "    # Save to JSON in records format (one JSON object per line)\n",
        "    df_results.to_json(output_json_filename, orient=\"records\", lines=True)\n",
        "    print(f\"Results saved to {output_json_filename}\")"
      ],
      "metadata": {
        "id": "lSa91s7Nl4oN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Comparison with xGES**"
      ],
      "metadata": {
        "id": "vo6Qg9btrhZL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_ges_method(X_data):\n",
        "    \"\"\"\n",
        "    Run the XGES algorithm on data X_data.\n",
        "\n",
        "    Returns a binary adjacency matrix such that est_adj[i,j]=1 indicates edge i → j.\n",
        "    \"\"\"\n",
        "    from xges import XGES\n",
        "    import numpy as np\n",
        "    import networkx as nx\n",
        "\n",
        "    try:\n",
        "        xges = XGES()\n",
        "        pdag = xges.fit(X_data)\n",
        "        dag = pdag.get_dag_extension()\n",
        "        est_adj = dag.to_adjacency_matrix()\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"XGES algorithm failed with error: {e}\")\n",
        "\n",
        "    # Remove any residual cycles.\n",
        "    G_nx = nx.DiGraph(est_adj)\n",
        "    remove_all_cycles(G_nx)\n",
        "    final_adj = nx.to_numpy_array(G_nx, dtype=int)\n",
        "    return final_adj\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    run_cell = True  # Set run_cell to True to execute\n",
        "    if not run_cell:\n",
        "        class StopExecutionWithMessage(Exception):\n",
        "            def __init__(self, message):\n",
        "                self.message = message\n",
        "            def _render_traceback_(self):\n",
        "                print(self.message)\n",
        "                return []\n",
        "        raise StopExecutionWithMessage(\"Execution halted.\")\n",
        "\n",
        "    # Experimental configurations\n",
        "    var_settings = [500]       # Number of variables\n",
        "    dag_types = ['ER', 'SF']           # ER: Erdős–Rényi, SF: Scale‑Free\n",
        "    sem_types = ['linear', 'non-linear']\n",
        "    noise_types = ['gaussian', 'exponential', 'laplace']  # Use \"laplace\" for Laplace noise\n",
        "    n_samples = 500                    # Number of samples per experiment\n",
        "    weight_scale = 1.0\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for n_vars in var_settings:\n",
        "        # Set edge probability for an expected average degree of ~2\n",
        "        edge_prob = 2.0 / (n_vars - 1)\n",
        "        for dag_type in dag_types:\n",
        "            # Generate the true DAG\n",
        "            if dag_type == 'ER':\n",
        "                true_adj = generate_erdos_renyi_dag(n_vars, edge_prob)\n",
        "            elif dag_type == 'SF':\n",
        "                true_adj = generate_scale_free_dag(n_vars)\n",
        "            else:\n",
        "                continue\n",
        "\n",
        "            for sem_type in sem_types:\n",
        "                for noise in noise_types:\n",
        "                    config_str = f\"n_vars={n_vars}, DAG={dag_type}, SEM={sem_type}, Noise={noise}\"\n",
        "                    print(\"\\n[Config] \" + config_str)\n",
        "                    # Simulate data and obtain true weight matrix\n",
        "                    if sem_type == 'linear':\n",
        "                        X_data, W_true = simulate_sem_with_weights(true_adj, n_samples,\n",
        "                                                                   noise_type=noise,\n",
        "                                                                   weight_scale=weight_scale)\n",
        "                    else:  # non-linear\n",
        "                        X_data, W_true = simulate_non_linear_sem_with_weights(true_adj, n_samples,\n",
        "                                                                              noise_type=noise,\n",
        "                                                                              weight_scale=weight_scale,\n",
        "                                                                              non_linear_fn=np.tanh)\n",
        "                    # Create normalized data (standardization: zero mean, unit std)\n",
        "                    X_data_norm = (X_data - X_data.mean(axis=0)) / (X_data.std(axis=0) + 1e-8)\n",
        "                    # ---------------------------\n",
        "                    # Run GES Method\n",
        "                    try:\n",
        "                        t0 = time.time()\n",
        "                        est_adj_ges = run_ges_method(X_data)\n",
        "                        runtime_ges = time.time() - t0\n",
        "                        shd_ges = shd(true_adj, est_adj_ges)\n",
        "                        sid_ges = sid(true_adj, est_adj_ges)\n",
        "                        ate_rmse_ges = rmse_ate(W_true, est_adj_ges.astype(float))\n",
        "                    except Exception as e:\n",
        "                        runtime_ges = np.nan\n",
        "                        shd_ges = np.nan\n",
        "                        sid_ges = np.nan\n",
        "                        ate_rmse_ges = np.nan\n",
        "                        print(\"  GES method failed with error:\", e)\n",
        "\n",
        "                    results.append({\n",
        "                        'Method': 'GES',\n",
        "                        'n_vars': n_vars,\n",
        "                        'DAG_type': dag_type,\n",
        "                        'SEM_type': sem_type,\n",
        "                        'Noise': noise,\n",
        "                        'Runtime_sec': runtime_ges,\n",
        "                        'SHD': shd_ges,\n",
        "                        'SID': sid_ges,\n",
        "                        'ATE_RMSE': ate_rmse_ges\n",
        "                    })\n",
        "\n",
        "                    # ---------------------------\n",
        "                    # Run DAG‑Boosting (FLAR) Method\n",
        "                    try:\n",
        "                        t0 = time.time()\n",
        "                        est_adj_dagboost, trainer = flar(X_data, threshold=0.3, verbose=False)\n",
        "                        runtime_dagboost = time.time() - t0\n",
        "                        shd_dagboost = shd(true_adj, est_adj_dagboost)\n",
        "                        sid_dagboost = sid(true_adj, est_adj_dagboost)\n",
        "                        ate_rmse_dagboost = rmse_ate(W_true, est_adj_dagboost.astype(float))\n",
        "                    except Exception as e:\n",
        "                        runtime_dagboost = np.nan\n",
        "                        shd_dagboost = np.nan\n",
        "                        sid_dagboost = np.nan\n",
        "                        ate_rmse_dagboost = np.nan\n",
        "                        print(\"  DAG‑Boosting method failed with error:\", e)\n",
        "\n",
        "                    results.append({\n",
        "                        'Method': 'DAGBoost',\n",
        "                        'n_vars': n_vars,\n",
        "                        'DAG_type': dag_type,\n",
        "                        'SEM_type': sem_type,\n",
        "                        'Noise': noise,\n",
        "                        'Runtime_sec': runtime_dagboost,\n",
        "                        'SHD': shd_dagboost,\n",
        "                        'SID': sid_dagboost,\n",
        "                        'ATE_RMSE': ate_rmse_dagboost\n",
        "                    })\n",
        "\n",
        "    # Compile and print results as a DataFrame\n",
        "    df_results = pd.DataFrame(results)\n",
        "    print(\"\\n===== Comparison Results =====\")\n",
        "    print(df_results)\n",
        "\n",
        "    # Save the results DataFrame as CSV and JSON files\n",
        "    output_csv_filename = \"GES_500_results.csv\"\n",
        "    output_json_filename = \"GES_500_results.json\"\n",
        "\n",
        "    # Save to CSV (with headers, no index)\n",
        "    df_results.to_csv(output_csv_filename, index=False)\n",
        "    print(f\"Results saved to {output_csv_filename}\")\n",
        "\n",
        "    # Save to JSON in records format (one JSON object per line)\n",
        "    df_results.to_json(output_json_filename, orient=\"records\", lines=True)\n",
        "    print(f\"Results saved to {output_json_filename}\")"
      ],
      "metadata": {
        "id": "GrYan7CZrju1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Comparison with SDCD**"
      ],
      "metadata": {
        "id": "TDRBdex_5KAc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sdcd\n",
        "!pip install wandb==0.15.10\n",
        "!pip install protobuf==3.20.3"
      ],
      "metadata": {
        "id": "GYIEvlSP7vAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sdcd.models import SDCD\n",
        "from sdcd.utils import create_intervention_dataset\n",
        "\n",
        "def run_sdcd_method(n_vars):\n",
        "\n",
        "  # Simulate Data\n",
        "  from sdcd.simulated_data import random_model_gaussian_global_variance # For demonstration\n",
        "\n",
        "  n = 500\n",
        "  n_per_intervention = 50\n",
        "  d = n_vars\n",
        "  n_edges = int(d*(2.0 / (n_vars - 1)))\n",
        "\n",
        "  true_causal_model = random_model_gaussian_global_variance(\n",
        "      d,\n",
        "      n_edges,\n",
        "      dag_type=\"ER\",\n",
        "      scale=0.5,\n",
        "      hard=True,\n",
        "  )\n",
        "  X_df = true_causal_model.generate_dataframe_from_all_distributions(\n",
        "      n_samples_control=n,\n",
        "      n_samples_per_intervention=n_per_intervention,\n",
        "  )\n",
        "  X_df.iloc[:, :-1] = (X_df.iloc[:, :-1] - X_df.iloc[:, :-1].mean()) / X_df.iloc[\n",
        "      :, :-1\n",
        "  ].std() # Normalize the data\n",
        "\n",
        "  X_dataset = create_intervention_dataset(X_df, perturbation_colname=\"perturbation_label\")\n",
        "  model = SDCD()\n",
        "  model.train(X_dataset)\n",
        "  adj_matrix = model.get_adjacency_matrix(threshold=True)\n",
        "  return adj_matrix\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    run_cell = True  # Set this to True to execute the comparison\n",
        "    if not run_cell:\n",
        "        class StopExecution(Exception):\n",
        "            def __init__(self, msg):\n",
        "                self.msg = msg\n",
        "            def _render_traceback_(self):\n",
        "                print(self.msg)\n",
        "                return []\n",
        "        raise StopExecution(\"Execution halted.\")\n",
        "\n",
        "    # Experimental configurations\n",
        "    var_settings = [500]       # Number of variables\n",
        "    dag_types = ['ER']#, 'SF']           # DAG types: Erdős–Rényi, Scale‑Free\n",
        "    sem_types = ['linear']#, 'non-linear']\n",
        "    noise_types = ['gaussian']#, 'exponential', 'laplace']  # Use \"laplace\" for Laplace noise\n",
        "    n_samples = 500                    # Samples per experiment\n",
        "    weight_scale = 1.0\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for n_vars in var_settings:\n",
        "        edge_prob = 2.0 / (n_vars - 1)  # Expected average degree ~2\n",
        "        for dag_type in dag_types:\n",
        "            # Generate true DAG\n",
        "            if dag_type == 'ER':\n",
        "                true_adj = generate_erdos_renyi_dag(n_vars, edge_prob)\n",
        "            elif dag_type == 'SF':\n",
        "                true_adj = generate_scale_free_dag(n_vars)\n",
        "            else:\n",
        "                continue\n",
        "            for sem_type in sem_types:\n",
        "                for noise in noise_types:\n",
        "                    config_str = f\"n_vars={n_vars}, DAG={dag_type}, SEM={sem_type}, Noise={noise}\"\n",
        "                    print(\"\\n[Config] \" + config_str)\n",
        "                    # Simulate data and get true weight matrix\n",
        "                    if sem_type == 'linear':\n",
        "                        X_data, W_true = simulate_sem_with_weights(true_adj, n_samples,\n",
        "                                                                   noise_type=noise,\n",
        "                                                                   weight_scale=weight_scale)\n",
        "                    else:\n",
        "                        X_data, W_true = simulate_non_linear_sem_with_weights(true_adj, n_samples,\n",
        "                                                                              noise_type=noise,\n",
        "                                                                              weight_scale=weight_scale,\n",
        "                                                                              non_linear_fn=np.tanh)\n",
        "                    # Create normalized data (standardization: zero mean, unit std)\n",
        "                    X_data_norm = (X_data - X_data.mean(axis=0)) / (X_data.std(axis=0) + 1e-8)\n",
        "                    # ---------------------------\n",
        "                    # Run SDCD Method\n",
        "                    try:\n",
        "                        t0 = time.time()\n",
        "                        est_adj_sdcd = run_sdcd_method(n_vars)\n",
        "                        runtime_sdcd = time.time() - t0\n",
        "                        shd_sdcd = shd(true_adj, est_adj_sdcd)\n",
        "                        sid_sdcd = sid(true_adj, est_adj_sdcd)\n",
        "                        ate_rmse_sdcd = rmse_ate(W_true, est_adj_sdcd.astype(float))\n",
        "                    except Exception as e:\n",
        "                        runtime_sdcd = np.nan\n",
        "                        shd_sdcd = np.nan\n",
        "                        sid_sdcd = np.nan\n",
        "                        ate_rmse_sdcd = np.nan\n",
        "                        print(\"  SDCD method failed with error:\", e)\n",
        "\n",
        "                    results.append({\n",
        "                        'Method': 'SDCD',\n",
        "                        'n_vars': n_vars,\n",
        "                        'DAG_type': dag_type,\n",
        "                        'SEM_type': sem_type,\n",
        "                        'Noise': noise,\n",
        "                        'Runtime_sec': runtime_sdcd,\n",
        "                        'SHD': shd_sdcd,\n",
        "                        'SID': sid_sdcd,\n",
        "                        'ATE_RMSE': ate_rmse_sdcd\n",
        "                    })\n",
        "\n",
        "                    # ---------------------------\n",
        "                    # Run DAG‑Boosting (FLAR) Method\n",
        "                    try:\n",
        "                        t0 = time.time()\n",
        "                        est_adj_dagboost, trainer = flar(X_data, threshold=0.3, verbose=False)\n",
        "                        runtime_dagboost = time.time() - t0\n",
        "                        shd_dagboost = shd(true_adj, est_adj_dagboost)\n",
        "                        sid_dagboost = sid(true_adj, est_adj_dagboost)\n",
        "                        ate_rmse_dagboost = rmse_ate(W_true, est_adj_dagboost.astype(float))\n",
        "                    except Exception as e:\n",
        "                        runtime_dagboost = np.nan\n",
        "                        shd_dagboost = np.nan\n",
        "                        sid_dagboost = np.nan\n",
        "                        ate_rmse_dagboost = np.nan\n",
        "                        print(\"  DAG‑Boosting method failed with error:\", e)\n",
        "\n",
        "                    results.append({\n",
        "                        'Method': 'DAGBoost',\n",
        "                        'n_vars': n_vars,\n",
        "                        'DAG_type': dag_type,\n",
        "                        'SEM_type': sem_type,\n",
        "                        'Noise': noise,\n",
        "                        'Runtime_sec': runtime_dagboost,\n",
        "                        'SHD': shd_dagboost,\n",
        "                        'SID': sid_dagboost,\n",
        "                        'ATE_RMSE': ate_rmse_dagboost\n",
        "                    })\n",
        "\n",
        "    df_results = pd.DataFrame(results)\n",
        "    print(\"\\n===== Comparison Results =====\")\n",
        "    print(df_results)\n",
        "\n",
        "    # Save the results DataFrame as CSV and JSON files\n",
        "    output_csv_filename = \"SDCD_500_results.csv\"\n",
        "    output_json_filename = \"SDCD_500_results.json\"\n",
        "\n",
        "    # Save to CSV (with headers, no index)\n",
        "    df_results.to_csv(output_csv_filename, index=False)\n",
        "    print(f\"Results saved to {output_csv_filename}\")\n",
        "\n",
        "    # Save to JSON in records format (one JSON object per line)\n",
        "    df_results.to_json(output_json_filename, orient=\"records\", lines=True)\n",
        "    print(f\"Results saved to {output_json_filename}\")"
      ],
      "metadata": {
        "id": "xq1QfnzB5Mfn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Comparison with SAM (Reviewer Configuration = 50 nodes)**"
      ],
      "metadata": {
        "id": "PhDl-mF7AYhN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "\"\"\"\n",
        "Complete Script to Compare SAM and DAG‑Boosting (FLAR)\n",
        "across metrics (SID, SHD, ATE_RMSE, runtime) for variable counts (e.g., 10, 50, 100).\n",
        "Evaluations are done for:\n",
        "  • DAG types: Erdős–Rényi (ER) and Scale‑Free (SF)\n",
        "  • SEM types: linear and non‑linear\n",
        "  • Noise types: gaussian, exponential, and laplace\n",
        "\n",
        "Make sure to install dependencies:\n",
        "    pip install numpy pandas torch networkx causal-learn statsmodels\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import networkx as nx\n",
        "import time\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 1. DAG Generation and Data Simulation Functions\n",
        "# ------------------------------------------------\n",
        "\n",
        "def generate_erdos_renyi_dag(num_nodes, edge_prob):\n",
        "    \"\"\"\n",
        "    Generate an Erdős–Rényi random DAG with given edge probability.\n",
        "    \"\"\"\n",
        "    perm = np.random.permutation(num_nodes)\n",
        "    adj = np.zeros((num_nodes, num_nodes), dtype=int)\n",
        "    for i in range(num_nodes):\n",
        "        for j in range(i+1, num_nodes):\n",
        "            if np.random.rand() < edge_prob:\n",
        "                adj[perm[i], perm[j]] = 1\n",
        "    return adj\n",
        "\n",
        "def remove_all_cycles(G):\n",
        "    \"\"\"\n",
        "    Remove edges from cycles until the graph is a DAG.\n",
        "    Operates in-place on a networkx DiGraph.\n",
        "    \"\"\"\n",
        "    while True:\n",
        "        try:\n",
        "            cycle_edges = nx.find_cycle(G, orientation='original')\n",
        "            for (u, v, _) in cycle_edges:\n",
        "                G.remove_edge(u, v)\n",
        "        except nx.NetworkXNoCycle:\n",
        "            break\n",
        "    return G\n",
        "\n",
        "def generate_scale_free_dag(num_nodes):\n",
        "    \"\"\"\n",
        "    Generate a scale‑free DAG: generate a scale‑free network and then remove cycles.\n",
        "    \"\"\"\n",
        "    G = nx.scale_free_graph(num_nodes, seed=None)\n",
        "    G_simple = nx.DiGraph(G)  # convert to a simple directed graph\n",
        "    G_simple.remove_edges_from(nx.selfloop_edges(G_simple))\n",
        "    G_dag = remove_all_cycles(G_simple.copy())\n",
        "    adj = nx.to_numpy_array(G_dag, dtype=int)\n",
        "    np.fill_diagonal(adj, 0)\n",
        "    return adj\n",
        "\n",
        "def simulate_sem_with_weights(adjacency_matrix, n_samples, noise_type='gaussian',\n",
        "                              weight_scale=1.0, random_state=None):\n",
        "    \"\"\"\n",
        "    Simulate data from a linear SEM:\n",
        "         X_j = sum_{i in Pa(j)} W_{i,j} * X_i + noise_j\n",
        "    Returns simulated data X and the true weight matrix W.\n",
        "    \"\"\"\n",
        "    if random_state is not None:\n",
        "        np.random.seed(random_state)\n",
        "    d = adjacency_matrix.shape[0]\n",
        "    G = nx.DiGraph(adjacency_matrix)\n",
        "    topo_order = list(nx.topological_sort(G))\n",
        "    W = np.zeros((d,d))\n",
        "    for i in range(d):\n",
        "        for j in range(d):\n",
        "            if adjacency_matrix[i, j] == 1:\n",
        "                W[i, j] = np.random.normal(loc=0.0, scale=weight_scale)\n",
        "    X = np.zeros((n_samples, d))\n",
        "    for s in range(n_samples):\n",
        "        for node in topo_order:\n",
        "            parents = np.where(adjacency_matrix[:, node]==1)[0]\n",
        "            parents_sum = np.sum(W[parents, node] * X[s, parents])\n",
        "            if noise_type.lower()=='gaussian':\n",
        "                noise = np.random.normal(loc=0.0, scale=1.0)\n",
        "            elif noise_type.lower()=='laplace':\n",
        "                noise = np.random.laplace(loc=0.0, scale=1.0)\n",
        "            elif noise_type.lower()=='exponential':\n",
        "                noise = np.random.exponential(scale=1.0)\n",
        "            else:\n",
        "                raise ValueError(\"noise_type must be one of 'gaussian', 'laplace', or 'exponential'\")\n",
        "            X[s, node] = parents_sum + noise\n",
        "    return X, W\n",
        "\n",
        "def simulate_non_linear_sem_with_weights(adjacency_matrix, n_samples, noise_type='gaussian',\n",
        "                                           weight_scale=1.0, random_state=None, non_linear_fn=np.tanh):\n",
        "    \"\"\"\n",
        "    Simulate data from a non-linear SEM:\n",
        "         X_j = f( sum_{i in Pa(j)} W_{i,j} * X_i ) + noise_j\n",
        "    Returns simulated data X and the true weight matrix W.\n",
        "    \"\"\"\n",
        "    if random_state is not None:\n",
        "        np.random.seed(random_state)\n",
        "    d = adjacency_matrix.shape[0]\n",
        "    G = nx.DiGraph(adjacency_matrix)\n",
        "    topo_order = list(nx.topological_sort(G))\n",
        "    W = np.zeros((d,d))\n",
        "    for i in range(d):\n",
        "        for j in range(d):\n",
        "            if adjacency_matrix[i, j] == 1:\n",
        "                W[i, j] = np.random.normal(loc=0.0, scale=weight_scale)\n",
        "    X = np.zeros((n_samples, d))\n",
        "    for s in range(n_samples):\n",
        "        for node in topo_order:\n",
        "            parents = np.where(adjacency_matrix[:, node]==1)[0]\n",
        "            parents_sum = np.sum(W[parents, node] * X[s, parents])\n",
        "            nl_term = non_linear_fn(parents_sum)\n",
        "            if noise_type.lower()=='gaussian':\n",
        "                noise = np.random.normal(loc=0.0, scale=1.0)\n",
        "            elif noise_type.lower()=='laplace':\n",
        "                noise = np.random.laplace(loc=0.0, scale=1.0)\n",
        "            elif noise_type.lower()=='exponential':\n",
        "                noise = np.random.exponential(scale=1.0)\n",
        "            else:\n",
        "                raise ValueError(\"noise_type must be one of 'gaussian', 'laplace', or 'exponential'\")\n",
        "            X[s, node] = nl_term + noise\n",
        "    return X, W\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 2. Evaluation Metrics: SHD, SID, ATE_RMSE\n",
        "# ------------------------------------------------\n",
        "\n",
        "def shd(true_adj, est_adj):\n",
        "    \"\"\"Compute Structural Hamming Distance.\"\"\"\n",
        "    return int(np.sum(true_adj != est_adj))\n",
        "\n",
        "def _compute_ancestors(adj):\n",
        "    G = nx.DiGraph(adj)\n",
        "    d = adj.shape[0]\n",
        "    ancestors = []\n",
        "    for node in range(d):\n",
        "        ancestors.append(set(nx.ancestors(G, node)))\n",
        "    return ancestors\n",
        "\n",
        "def sid(true_adj, est_adj):\n",
        "    \"\"\"Compute Structural Intervention Distance (SID).\"\"\"\n",
        "    true_anc = _compute_ancestors(true_adj)\n",
        "    est_anc = _compute_ancestors(est_adj)\n",
        "    d = true_adj.shape[0]\n",
        "    score = 0\n",
        "    for j in range(d):\n",
        "        score += len(true_anc[j].difference(est_anc[j])) + len(est_anc[j].difference(true_anc[j]))\n",
        "    return score\n",
        "\n",
        "def compute_total_effect_matrix(W):\n",
        "    d = W.shape[0]\n",
        "    I = np.eye(d)\n",
        "    try:\n",
        "        inv = np.linalg.inv(I - W)\n",
        "    except np.linalg.LinAlgError:\n",
        "        return np.zeros((d,d))\n",
        "    return inv - I\n",
        "\n",
        "def rmse_ate(W_true, W_est):\n",
        "    T_true = compute_total_effect_matrix(W_true)\n",
        "    T_est  = compute_total_effect_matrix(W_est)\n",
        "    return np.sqrt(np.mean((T_true - T_est)**2))\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 3. DAG‑Boosting (FLAR) Implementation (Same as Your Version)\n",
        "# ------------------------------------------------\n",
        "\n",
        "def squared_loss(x_true, x_pred):\n",
        "    return 0.5 * torch.mean((x_true - x_pred)**2)\n",
        "\n",
        "def dag_constraint(W):\n",
        "    d = W.shape[0]\n",
        "    WW = W * W\n",
        "    expm_WW = torch.matrix_exp(WW)\n",
        "    return torch.trace(expm_WW) - d\n",
        "\n",
        "def apply_mask(W, mask):\n",
        "    with torch.no_grad():\n",
        "        W *= mask\n",
        "        d = W.shape[0]\n",
        "        for i in range(d):\n",
        "            W[i, i] = 0.0\n",
        "\n",
        "def binarize_adjacency(W, threshold=0.3):\n",
        "    W_np = W.detach().cpu().numpy()\n",
        "    W_bin = (np.abs(W_np) > threshold).astype(float)\n",
        "    np.fill_diagonal(W_bin, 0.0)\n",
        "    return W_bin\n",
        "\n",
        "class WeakLearnerNN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=4):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class FunctionalBoostingModel(nn.Module):\n",
        "    def __init__(self, d, max_num_weak_learners=50, hidden_dim=40):\n",
        "        super().__init__()\n",
        "        self.d = d\n",
        "        self.max_num_weak_learners = max_num_weak_learners\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.learners_for_var = [[] for _ in range(d)]\n",
        "        self.weak_learners = nn.ModuleList()\n",
        "        self.current_counts = [0]*d\n",
        "    def forward(self, X, W):\n",
        "        N, d = X.shape\n",
        "        device = X.device\n",
        "        Xhat = []\n",
        "        for i in range(d):\n",
        "            mask = W[i, :]\n",
        "            masked = X * mask  # Elementwise multiplication; broadcast mask along rows.\n",
        "            pred = torch.zeros((N, 1), dtype=X.dtype, device=device)\n",
        "            for learner in self.learners_for_var[i]:\n",
        "                pred += learner(masked)\n",
        "            Xhat.append(pred)\n",
        "        return torch.cat(Xhat, dim=1)\n",
        "    def add_weak_learner(self, i):\n",
        "        wl = WeakLearnerNN(input_dim=self.d, hidden_dim=self.hidden_dim)\n",
        "        self.weak_learners.append(wl)\n",
        "        self.learners_for_var[i].append(wl)\n",
        "        self.current_counts[i] += 1\n",
        "    def fit_new_weak_learner(self, i, X, residual, W, n_epochs=5, lr=0.01, verbose=False):\n",
        "        self.add_weak_learner(i)\n",
        "        wl = self.learners_for_var[i][-1]\n",
        "        optimizer = optim.Adam(wl.parameters(), lr=lr)\n",
        "        for epoch in range(n_epochs):\n",
        "            optimizer.zero_grad()\n",
        "            mask = W[i, :]\n",
        "            masked = X * mask\n",
        "            pred = wl(masked)\n",
        "            loss = torch.mean((pred - residual)**2)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            if verbose:\n",
        "                print(f\"    [Variable {i} Epoch {epoch}] Loss={loss.item():.6f}\")\n",
        "\n",
        "class DAGBoostingTrainer:\n",
        "    def __init__(self, d, adjacency_mask=None, lr_W=0.01, lambda_h=5.0, alpha_init=0.0,\n",
        "                 max_iter=5, max_num_weak_learners=50, hidden_dim=40, tol=1e-4,\n",
        "                 patience=2, device=torch.device(\"cpu\")):\n",
        "        self.d = d\n",
        "        self.model = FunctionalBoostingModel(d, max_num_weak_learners, hidden_dim).to(device)\n",
        "        W_init = 0.01 * torch.randn(d, d, device=device)\n",
        "        for i in range(d):\n",
        "            W_init[i, i] = 0.0\n",
        "        self.W = nn.Parameter(W_init)\n",
        "        if adjacency_mask is None:\n",
        "            adjacency_mask = np.ones((d, d), dtype=np.float32)\n",
        "            np.fill_diagonal(adjacency_mask, 0)\n",
        "        self.adjacency_mask = torch.tensor(adjacency_mask, dtype=torch.float32, device=device)\n",
        "        self.lambda_h = lambda_h\n",
        "        self.alpha = alpha_init\n",
        "        self.lr_W = lr_W\n",
        "        self.max_iter = max_iter\n",
        "        self.tol = tol\n",
        "        self.patience = patience\n",
        "        self.device = device\n",
        "        self.best_loss = float('inf')\n",
        "        self.no_improv_steps = 0\n",
        "        self.stop_early = False\n",
        "    def parameters(self):\n",
        "        return list(self.model.parameters()) + [self.W]\n",
        "    def apply_domain_mask_and_no_loops(self):\n",
        "        apply_mask(self.W, self.adjacency_mask)\n",
        "    def augmented_lagrangian_loss(self, X):\n",
        "        Xhat = self.model(X, self.W)\n",
        "        recon_loss = squared_loss(X, Xhat)\n",
        "        h = dag_constraint(self.W)\n",
        "        aug = self.alpha * h + 0.5 * self.lambda_h * (h ** 2)\n",
        "        return recon_loss + aug, recon_loss, h\n",
        "    def update_dual(self, h):\n",
        "        self.alpha = self.alpha + self.lambda_h * h.item()\n",
        "    def train(self, X, batch_size=128, n_inner_epochs=10, fit_new_learner_epochs=5, verbose=True):\n",
        "        if not isinstance(X, torch.Tensor):\n",
        "            X = torch.tensor(X, dtype=torch.float32, device=self.device)\n",
        "        else:\n",
        "            X = X.to(self.device)\n",
        "        N = X.shape[0]\n",
        "        for outer in range(self.max_iter):\n",
        "            indices = np.random.permutation(N)\n",
        "            batch_idx = indices[:batch_size]\n",
        "            X_batch = X[batch_idx]\n",
        "            with torch.no_grad():\n",
        "                Xhat = self.model(X_batch, self.W)\n",
        "                residual = X_batch - Xhat\n",
        "                mse = torch.mean(residual**2).item()\n",
        "            if verbose:\n",
        "                print(f\"Outer Iteration {outer+1}/{self.max_iter}, Batch MSE: {mse:.6f}\")\n",
        "            for i in range(self.d):\n",
        "                if self.model.current_counts[i] < self.model.max_num_weak_learners:\n",
        "                    res = residual[:, i:i+1]\n",
        "                    self.model.fit_new_weak_learner(i, X_batch, res, self.W,\n",
        "                                                     n_epochs=fit_new_learner_epochs, lr=0.01,\n",
        "                                                     verbose=False)\n",
        "            opt = optim.Adam(self.parameters(), lr=self.lr_W)\n",
        "            for inner in range(n_inner_epochs):\n",
        "                opt.zero_grad()\n",
        "                loss_total, loss_rec, h_val = self.augmented_lagrangian_loss(X_batch)\n",
        "                loss_total.backward()\n",
        "                opt.step()\n",
        "                self.apply_domain_mask_and_no_loops()\n",
        "            with torch.no_grad():\n",
        "                _, _, h_val = self.augmented_lagrangian_loss(X_batch)\n",
        "            self.update_dual(h_val)\n",
        "        return self.W.detach(), self.model\n",
        "    def get_binarized_adjacency(self, threshold=0.3):\n",
        "        return binarize_adjacency(self.W, threshold=threshold)\n",
        "\n",
        "def run_dagboost_method(X_data, threshold=0.3, max_iter=5, max_num_weak_learners=50,\n",
        "                        hidden_dim=40, lambda_h=5.0, verbose=False):\n",
        "    device = torch.device(\"cpu\")\n",
        "    d = X_data.shape[1]\n",
        "    adj_mask = np.ones((d, d), dtype=np.float32)\n",
        "    np.fill_diagonal(adj_mask, 0)\n",
        "    trainer = DAGBoostingTrainer(d, adjacency_mask=adj_mask, lr_W=0.01, lambda_h=lambda_h,\n",
        "                                 alpha_init=0.0, max_iter=max_iter,\n",
        "                                 max_num_weak_learners=max_num_weak_learners,\n",
        "                                 hidden_dim=hidden_dim, tol=1e-5, patience=2, device=device)\n",
        "    trainer.train(X_data, batch_size=128, n_inner_epochs=10, fit_new_learner_epochs=5, verbose=verbose)\n",
        "    W_bin = trainer.get_binarized_adjacency(threshold=threshold)\n",
        "    # Remove cycles\n",
        "    G = nx.DiGraph(W_bin)\n",
        "    remove_all_cycles(G)\n",
        "    return nx.to_numpy_array(G, dtype=int), trainer\n",
        "\n",
        "def flar(X_data, threshold=0.3, max_iter=3, max_num_weak_learners=50, hidden_dim=40,\n",
        "         lambda_h=5.0, verbose=False):\n",
        "    return run_dagboost_method(X_data, threshold=threshold, max_iter=max_iter,\n",
        "                               max_num_weak_learners=max_num_weak_learners,\n",
        "                               hidden_dim=hidden_dim, lambda_h=lambda_h, verbose=verbose)\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 4. SAM Method (using the cdt Package)\n",
        "# ------------------------------------------------\n",
        "\n",
        "def run_sam_method(X_data, nruns=1):\n",
        "    \"\"\"\n",
        "    Run the SAM algorithm from cdt.\n",
        "    Convert the simulated data (NumPy array) into a pandas DataFrame,\n",
        "    then run SAM. The output graph is converted to a binary adjacency matrix\n",
        "    where an edge i → j is denoted by a 1 at position [i, j].\n",
        "    \"\"\"\n",
        "    try:\n",
        "        from cdt.causality.graph import SAM\n",
        "        import cdt\n",
        "    except ImportError:\n",
        "        raise ImportError(\"cdt package is required for SAM. Install with 'pip install cdt'.\")\n",
        "\n",
        "\n",
        "    cdt.SETTINGS.GPU = 0  # use CPU\n",
        "    data_df = pd.DataFrame(X_data)\n",
        "    sam = SAM(nruns=nruns) #sam = SAM(nruns=1)\n",
        "    # SAM's output is a graph; we assume output_graph.edges() returns directed edges.\n",
        "    output_graph = sam.predict(data_df)\n",
        "    d = X_data.shape[1]\n",
        "    est_adj = np.zeros((d, d), dtype=int)\n",
        "    # For each edge in the graph, assume it is directed: if there is an edge from u to v,\n",
        "    # then set est_adj[u, v] = 1.\n",
        "    for (u, v) in output_graph.edges():\n",
        "        est_adj[u, v] = 1\n",
        "    # Optionally remove cycles.\n",
        "    G_nx = nx.DiGraph(est_adj)\n",
        "    remove_all_cycles(G_nx)\n",
        "    final_adj = nx.to_numpy_array(G_nx, dtype=int)\n",
        "    return final_adj\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 5. Main Comparison Loop: Testing Configurations\n",
        "# ------------------------------------------------\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    run_cell = True  # Set this to True to execute the comparison\n",
        "    if not run_cell:\n",
        "        class StopExecution(Exception):\n",
        "            def __init__(self, msg):\n",
        "                self.msg = msg\n",
        "            def _render_traceback_(self):\n",
        "                print(self.msg)\n",
        "                return []\n",
        "        raise StopExecution(\"Execution halted.\")\n",
        "\n",
        "    # Experimental configurations\n",
        "    var_settings = [50]#, 50, 100]       # Number of variables\n",
        "    dag_types = ['ER']#, 'SF']           # DAG types: Erdős–Rényi, Scale‑Free\n",
        "    sem_types = ['linear']#, 'non-linear']\n",
        "    noise_types = ['gaussian']#, 'exponential', 'laplace']  # Use \"laplace\" for Laplace noise\n",
        "    n_samples = 500                    # Samples per experiment\n",
        "    weight_scale = 1.0\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for n_vars in var_settings:\n",
        "        edge_prob = 2.0 / (n_vars - 1)  # Expected average degree ~2\n",
        "        for dag_type in dag_types:\n",
        "            # Generate true DAG\n",
        "            if dag_type == 'ER':\n",
        "                true_adj = generate_erdos_renyi_dag(n_vars, edge_prob)\n",
        "            elif dag_type == 'SF':\n",
        "                true_adj = generate_scale_free_dag(n_vars)\n",
        "            else:\n",
        "                continue\n",
        "            for sem_type in sem_types:\n",
        "                for noise in noise_types:\n",
        "                    config_str = f\"n_vars={n_vars}, DAG={dag_type}, SEM={sem_type}, Noise={noise}\"\n",
        "                    print(\"\\n[Config] \" + config_str)\n",
        "                    # Simulate data and get true weight matrix\n",
        "                    if sem_type == 'linear':\n",
        "                        X_data, W_true = simulate_sem_with_weights(true_adj, n_samples,\n",
        "                                                                   noise_type=noise,\n",
        "                                                                   weight_scale=weight_scale)\n",
        "                    else:\n",
        "                        X_data, W_true = simulate_non_linear_sem_with_weights(true_adj, n_samples,\n",
        "                                                                              noise_type=noise,\n",
        "                                                                              weight_scale=weight_scale,\n",
        "                                                                              non_linear_fn=np.tanh)\n",
        "                    # ---------------------------\n",
        "                    # Run SAM Method\n",
        "                    try:\n",
        "                        t0 = time.time()\n",
        "                        est_adj_sam = run_sam_method(X_data, nruns=1)\n",
        "                        runtime_sam = time.time() - t0\n",
        "                        shd_sam = shd(true_adj, est_adj_sam)\n",
        "                        sid_sam = sid(true_adj, est_adj_sam)\n",
        "                        ate_rmse_sam = rmse_ate(W_true, est_adj_sam.astype(float))\n",
        "                    except Exception as e:\n",
        "                        runtime_sam = np.nan\n",
        "                        shd_sam = np.nan\n",
        "                        sid_sam = np.nan\n",
        "                        ate_rmse_sam = np.nan\n",
        "                        print(\"  SAM method failed with error:\", e)\n",
        "\n",
        "                    results.append({\n",
        "                        'Method': 'SAM',\n",
        "                        'n_vars': n_vars,\n",
        "                        'DAG_type': dag_type,\n",
        "                        'SEM_type': sem_type,\n",
        "                        'Noise': noise,\n",
        "                        'Runtime_sec': runtime_sam,\n",
        "                        'SHD': shd_sam,\n",
        "                        'SID': sid_sam,\n",
        "                        'ATE_RMSE': ate_rmse_sam\n",
        "                    })\n",
        "\n",
        "                    # ---------------------------\n",
        "                    # Run DAG‑Boosting (FLAR) Method\n",
        "                    try:\n",
        "                        t0 = time.time()\n",
        "                        est_adj_dagboost, trainer = flar(X_data, threshold=0.3, verbose=False)\n",
        "                        runtime_dagboost = time.time() - t0\n",
        "                        shd_dagboost = shd(true_adj, est_adj_dagboost)\n",
        "                        sid_dagboost = sid(true_adj, est_adj_dagboost)\n",
        "                        ate_rmse_dagboost = rmse_ate(W_true, est_adj_dagboost.astype(float))\n",
        "                    except Exception as e:\n",
        "                        runtime_dagboost = np.nan\n",
        "                        shd_dagboost = np.nan\n",
        "                        sid_dagboost = np.nan\n",
        "                        ate_rmse_dagboost = np.nan\n",
        "                        print(\"  DAG‑Boosting method failed with error:\", e)\n",
        "\n",
        "                    results.append({\n",
        "                        'Method': 'DAGBoost',\n",
        "                        'n_vars': n_vars,\n",
        "                        'DAG_type': dag_type,\n",
        "                        'SEM_type': sem_type,\n",
        "                        'Noise': noise,\n",
        "                        'Runtime_sec': runtime_dagboost,\n",
        "                        'SHD': shd_dagboost,\n",
        "                        'SID': sid_dagboost,\n",
        "                        'ATE_RMSE': ate_rmse_dagboost\n",
        "                    })\n",
        "\n",
        "    df_results = pd.DataFrame(results)\n",
        "    print(\"\\n===== Comparison Results =====\")\n",
        "    print(df_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j046C_TFAbpo",
        "outputId": "386fe04e-6332-4b42-df45-09d67f3927ee"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Config] n_vars=50, DAG=ER, SEM=linear, Noise=gaussian\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Detecting 1 CUDA device(s).\n",
            "100%|██████████| 4000/4000 [17:46<00:00,  3.75it/s, disc=1.08e+4, gen=-1.08e+4, regul_loss=1.42, tot=-5.42e+5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Comparison Results =====\n",
            "     Method  n_vars DAG_type SEM_type     Noise  Runtime_sec  SHD  SID  \\\n",
            "0       SAM      50       ER   linear  gaussian  1067.943505   57   95   \n",
            "1  DAGBoost      50       ER   linear  gaussian     3.100763   60   98   \n",
            "\n",
            "   ATE_RMSE  \n",
            "0  0.242972  \n",
            "1  0.245429  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Normalized Linear Experiments**"
      ],
      "metadata": {
        "id": "wvK2l8i0mkWA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "\"\"\"\n",
        "Comprehensive Experiment for DAG‑Boosting (FLAR)\n",
        "\n",
        "This script compares your method across:\n",
        "  • SEM types: linear and non‑linear\n",
        "  • Node counts: 20, 50, 100\n",
        "  • DAG types: Erdős–Rényi (ER) and Scale‑Free (SF)\n",
        "  • Noise: gaussian, exponential, and laplace\n",
        "  • Data pre‑processing: raw vs. normalized (standardized)\n",
        "\n",
        "For each experiment the script:\n",
        "  - Simulates SEM data and returns data X and the true weight matrix W_true.\n",
        "  - Runs DAG‑Boosting (FLAR) on the data.\n",
        "  - Computes the estimated total effect matrix T = (I - W)⁻¹ - I and extracts an ATE estimate (using X0 as treatment and X_{n-1} as outcome).\n",
        "  - Thresholds the learned continuous W to recover a binary adjacency matrix.\n",
        "  - Calculates evaluation metrics: SHD, SID, and RMSE (ATE_RMSE).\n",
        "  - Records runtime.\n",
        "Results are compiled into a pandas DataFrame and printed.\n",
        "\n",
        "Dependencies:\n",
        "    pip install numpy pandas torch networkx statsmodels\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import networkx as nx\n",
        "import time\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# -------------------------------\n",
        "# 1. Data Simulation and DAG Generation\n",
        "# -------------------------------\n",
        "\n",
        "def generate_erdos_renyi_dag(num_nodes, edge_prob):\n",
        "    \"\"\"\n",
        "    Generate an Erdős–Rényi random DAG with a given edge probability.\n",
        "    \"\"\"\n",
        "    perm = np.random.permutation(num_nodes)\n",
        "    adj = np.zeros((num_nodes, num_nodes), dtype=int)\n",
        "    for i in range(num_nodes):\n",
        "        for j in range(i+1, num_nodes):\n",
        "            if np.random.rand() < edge_prob:\n",
        "                adj[perm[i], perm[j]] = 1\n",
        "    return adj\n",
        "\n",
        "def generate_scale_free_dag(num_nodes):\n",
        "    \"\"\"\n",
        "    Generate a scale‑free DAG: first generate a scale‑free network then remove cycles.\n",
        "    \"\"\"\n",
        "    G = nx.scale_free_graph(num_nodes, seed=None)\n",
        "    G_simple = nx.DiGraph(G)\n",
        "    G_simple.remove_edges_from(nx.selfloop_edges(G_simple))\n",
        "    G_dag = remove_all_cycles(G_simple.copy())\n",
        "    adj = nx.to_numpy_array(G_dag, dtype=int)\n",
        "    np.fill_diagonal(adj, 0)\n",
        "    return adj\n",
        "\n",
        "def remove_all_cycles(G):\n",
        "    \"\"\"\n",
        "    Remove edges from cycles until G is a DAG. Modifies G in-place.\n",
        "    \"\"\"\n",
        "    while True:\n",
        "        try:\n",
        "            cycle = nx.find_cycle(G, orientation='original')\n",
        "            for (u, v, _) in cycle:\n",
        "                G.remove_edge(u, v)\n",
        "        except nx.NetworkXNoCycle:\n",
        "            break\n",
        "    return G\n",
        "\n",
        "def simulate_sem_with_weights(adjacency_matrix, n_samples, noise_type='gaussian', weight_scale=1.0, random_state=None):\n",
        "    \"\"\"\n",
        "    Simulate data from a linear SEM:\n",
        "         X_j = sum_{i in Pa(j)} W_ij * X_i + noise_j\n",
        "    Returns:\n",
        "         X (n_samples x num_vars) and the true weight matrix W.\n",
        "    \"\"\"\n",
        "    if random_state is not None:\n",
        "        np.random.seed(random_state)\n",
        "    d = adjacency_matrix.shape[0]\n",
        "    G = nx.DiGraph(adjacency_matrix)\n",
        "    topo_order = list(nx.topological_sort(G))\n",
        "    W = np.zeros((d, d))\n",
        "    for i in range(d):\n",
        "        for j in range(d):\n",
        "            if adjacency_matrix[i, j] == 1:\n",
        "                W[i, j] = np.random.normal(loc=0.0, scale=weight_scale)\n",
        "    X = np.zeros((n_samples, d))\n",
        "    for s in range(n_samples):\n",
        "        for node in topo_order:\n",
        "            parents = np.where(adjacency_matrix[:, node] == 1)[0]\n",
        "            val = np.sum(W[parents, node] * X[s, parents])\n",
        "            if noise_type.lower() == 'gaussian':\n",
        "                noise = np.random.normal(loc=0.0, scale=1.0)\n",
        "            elif noise_type.lower() == 'exponential':\n",
        "                noise = np.random.exponential(scale=1.0)\n",
        "            elif noise_type.lower() == 'laplace' or noise_type.lower()=='laplacian':\n",
        "                noise = np.random.laplace(loc=0.0, scale=1.0)\n",
        "            else:\n",
        "                raise ValueError(\"noise_type must be gaussian, exponential, or laplace\")\n",
        "            X[s, node] = val + noise\n",
        "    return X, W\n",
        "\n",
        "def simulate_non_linear_sem_with_weights(adjacency_matrix, n_samples, noise_type='gaussian',\n",
        "                                           weight_scale=1.0, random_state=None, non_linear_fn=np.tanh):\n",
        "    \"\"\"\n",
        "    Simulate data from a non-linear SEM:\n",
        "         X_j = f(sum_{i in Pa(j)} W_ij * X_i) + noise_j, with f non-linear (e.g. tanh)\n",
        "    Returns:\n",
        "         X and the true weight matrix W.\n",
        "    \"\"\"\n",
        "    if random_state is not None:\n",
        "        np.random.seed(random_state)\n",
        "    d = adjacency_matrix.shape[0]\n",
        "    G = nx.DiGraph(adjacency_matrix)\n",
        "    topo_order = list(nx.topological_sort(G))\n",
        "    W = np.zeros((d, d))\n",
        "    for i in range(d):\n",
        "        for j in range(d):\n",
        "            if adjacency_matrix[i, j] == 1:\n",
        "                W[i, j] = np.random.normal(loc=0.0, scale=weight_scale)\n",
        "    X = np.zeros((n_samples, d))\n",
        "    for s in range(n_samples):\n",
        "        for node in topo_order:\n",
        "            parents = np.where(adjacency_matrix[:, node]==1)[0]\n",
        "            val = np.sum(W[parents, node] * X[s, parents])\n",
        "            nl_val = non_linear_fn(val)\n",
        "            if noise_type.lower() == 'gaussian':\n",
        "                noise = np.random.normal(loc=0.0, scale=1.0)\n",
        "            elif noise_type.lower() == 'exponential':\n",
        "                noise = np.random.exponential(scale=1.0)\n",
        "            elif noise_type.lower() == 'laplace' or noise_type.lower()=='laplacian':\n",
        "                noise = np.random.laplace(loc=0.0, scale=1.0)\n",
        "            else:\n",
        "                raise ValueError(\"noise_type must be gaussian, exponential, or laplace\")\n",
        "            X[s, node] = nl_val + noise\n",
        "    return X, W\n",
        "\n",
        "# -------------------------------\n",
        "# 2. Evaluation Metrics\n",
        "# -------------------------------\n",
        "\n",
        "def shd(true_adj, est_adj):\n",
        "    \"\"\"Structural Hamming Distance.\"\"\"\n",
        "    return int(np.sum(true_adj != est_adj))\n",
        "\n",
        "def _compute_ancestors(adj):\n",
        "    G = nx.DiGraph(adj)\n",
        "    d = adj.shape[0]\n",
        "    ancestors = []\n",
        "    for node in range(d):\n",
        "        ancestors.append(set(nx.ancestors(G, node)))\n",
        "    return ancestors\n",
        "\n",
        "def sid(true_adj, est_adj):\n",
        "    \"\"\"Structural Intervention Distance.\"\"\"\n",
        "    true_anc = _compute_ancestors(true_adj)\n",
        "    est_anc = _compute_ancestors(est_adj)\n",
        "    d = true_adj.shape[0]\n",
        "    score = 0\n",
        "    for j in range(d):\n",
        "        score += len(true_anc[j].difference(est_anc[j])) + len(est_anc[j].difference(true_anc[j]))\n",
        "    return score\n",
        "\n",
        "def compute_total_effect_matrix(W):\n",
        "    \"\"\"\n",
        "    Compute T = (I - W)^{-1} - I.\n",
        "    \"\"\"\n",
        "    d = W.shape[0]\n",
        "    I = np.eye(d)\n",
        "    try:\n",
        "        inv = np.linalg.inv(I - W)\n",
        "    except np.linalg.LinAlgError:\n",
        "        inv = I\n",
        "    return inv - I\n",
        "\n",
        "def rmse_ate(W_true, W_est):\n",
        "    \"\"\"\n",
        "    Compute RMSE of the total effect matrices.\n",
        "    \"\"\"\n",
        "    T_true = compute_total_effect_matrix(W_true)\n",
        "    T_est  = compute_total_effect_matrix(W_est)\n",
        "    return np.sqrt(np.mean((T_true - T_est)**2))\n",
        "\n",
        "def threshold_W(W, thr=0.3):\n",
        "    W_bin = (np.abs(W) > thr).astype(int)\n",
        "    np.fill_diagonal(W_bin, 0)\n",
        "    return W_bin\n",
        "\n",
        "# -------------------------------\n",
        "# 3. DAG‑Boosting (FLAR) Implementation\n",
        "# -------------------------------\n",
        "\n",
        "def squared_loss(x_true, x_pred):\n",
        "    return 0.5 * torch.mean((x_true - x_pred)**2)\n",
        "\n",
        "def dag_constraint(W):\n",
        "    d = W.shape[0]\n",
        "    WW = W * W\n",
        "    expm_WW = torch.matrix_exp(WW)\n",
        "    return torch.trace(expm_WW) - d\n",
        "\n",
        "def apply_mask(W, mask):\n",
        "    with torch.no_grad():\n",
        "        W *= mask\n",
        "        d = W.shape[0]\n",
        "        for i in range(d):\n",
        "            W[i, i] = 0.0\n",
        "\n",
        "def binarize_adjacency(W, threshold=0.3):\n",
        "    W_np = W.detach().cpu().numpy()\n",
        "    W_bin = (np.abs(W_np) > threshold).astype(float)\n",
        "    np.fill_diagonal(W_bin, 0.0)\n",
        "    return W_bin\n",
        "\n",
        "class WeakLearnerNN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=4):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class FunctionalBoostingModel(nn.Module):\n",
        "    def __init__(self, d, max_num_weak_learners=2, hidden_dim=4):\n",
        "        super().__init__()\n",
        "        self.d = d\n",
        "        self.max_num_weak_learners = max_num_weak_learners\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.learners_for_var = [[] for _ in range(d)]\n",
        "        self.weak_learners = nn.ModuleList()\n",
        "        self.current_counts = [0]*d\n",
        "    def forward(self, X, W):\n",
        "        N, d = X.shape\n",
        "        device = X.device\n",
        "        preds = []\n",
        "        for i in range(d):\n",
        "            mask = W[i, :]\n",
        "            masked = X * mask\n",
        "            pred = torch.zeros((N, 1), dtype=X.dtype, device=device)\n",
        "            for learner in self.learners_for_var[i]:\n",
        "                pred += learner(masked)\n",
        "            preds.append(pred)\n",
        "        return torch.cat(preds, dim=1)\n",
        "    def add_weak_learner(self, i):\n",
        "        wl = WeakLearnerNN(input_dim=self.d, hidden_dim=self.hidden_dim)\n",
        "        self.weak_learners.append(wl)\n",
        "        self.learners_for_var[i].append(wl)\n",
        "        self.current_counts[i] += 1\n",
        "    def fit_new_weak_learner(self, i, X, residual, W, n_epochs=5, lr=0.01, verbose=False):\n",
        "        self.add_weak_learner(i)\n",
        "        wl = self.learners_for_var[i][-1]\n",
        "        optimizer = optim.Adam(wl.parameters(), lr=lr)\n",
        "        for epoch in range(n_epochs):\n",
        "            optimizer.zero_grad()\n",
        "            mask = W[i, :]\n",
        "            masked = X * mask\n",
        "            pred = wl(masked)\n",
        "            loss = torch.mean((pred - residual)**2)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            if verbose:\n",
        "                print(f\"    [Var {i} Epoch {epoch}] Loss={loss.item():.6f}\")\n",
        "\n",
        "class DAGBoostingTrainer:\n",
        "    def __init__(self, d, adjacency_mask=None, lr_W=0.01, lambda_h=5.0, alpha_init=0.0,\n",
        "                 max_iter=3, max_num_weak_learners=2, hidden_dim=4, tol=1e-4,\n",
        "                 patience=2, device=torch.device(\"cpu\")):\n",
        "        self.d = d\n",
        "        self.model = FunctionalBoostingModel(d, max_num_weak_learners, hidden_dim).to(device)\n",
        "        W_init = 0.01 * torch.randn(d, d, device=device)\n",
        "        for i in range(d):\n",
        "            W_init[i, i] = 0.0\n",
        "        self.W = nn.Parameter(W_init)\n",
        "        if adjacency_mask is None:\n",
        "            adjacency_mask = np.ones((d, d), dtype=np.float32)\n",
        "            np.fill_diagonal(adjacency_mask, 0)\n",
        "        self.adjacency_mask = torch.tensor(adjacency_mask, dtype=torch.float32, device=device)\n",
        "        self.lambda_h = lambda_h\n",
        "        self.alpha = alpha_init\n",
        "        self.lr_W = lr_W\n",
        "        self.max_iter = max_iter\n",
        "        self.tol = tol\n",
        "        self.patience = patience\n",
        "        self.device = device\n",
        "        self.best_loss = float('inf')\n",
        "        self.no_improv_steps = 0\n",
        "        self.stop_early = False\n",
        "    def parameters(self):\n",
        "        return list(self.model.parameters()) + [self.W]\n",
        "    def apply_domain_mask_and_no_loops(self):\n",
        "        apply_mask(self.W, self.adjacency_mask)\n",
        "    def augmented_lagrangian_loss(self, X):\n",
        "        Xhat = self.model(X, self.W)\n",
        "        rec_loss = squared_loss(X, Xhat)\n",
        "        h = dag_constraint(self.W)\n",
        "        aug = self.alpha * h + 0.5 * self.lambda_h * (h**2)\n",
        "        return rec_loss + aug, rec_loss, h\n",
        "    def update_dual(self, h):\n",
        "        self.alpha = self.alpha + self.lambda_h * h.item()\n",
        "    def train(self, X, batch_size=128, n_inner_epochs=10, fit_new_learner_epochs=5, verbose=True):\n",
        "        if not isinstance(X, torch.Tensor):\n",
        "            X = torch.tensor(X, dtype=torch.float32, device=self.device)\n",
        "        else:\n",
        "            X = X.to(self.device)\n",
        "        N = X.shape[0]\n",
        "        for outer in range(self.max_iter):\n",
        "            indices = np.random.permutation(N)\n",
        "            batch_idx = indices[:batch_size]\n",
        "            X_batch = X[batch_idx]\n",
        "            with torch.no_grad():\n",
        "                Xhat = self.model(X_batch, self.W)\n",
        "                residual = X_batch - Xhat\n",
        "                mse = torch.mean(residual**2).item()\n",
        "            if verbose:\n",
        "                print(f\"Outer Iteration {outer+1}/{self.max_iter}, Batch MSE: {mse:.6f}\")\n",
        "            for i in range(self.d):\n",
        "                if self.model.current_counts[i] < self.model.max_num_weak_learners:\n",
        "                    res = residual[:, i:i+1]\n",
        "                    self.model.fit_new_weak_learner(i, X_batch, res, self.W,\n",
        "                                                     n_epochs=fit_new_learner_epochs,\n",
        "                                                     lr=0.01, verbose=False)\n",
        "            opt = optim.Adam(self.parameters(), lr=self.lr_W)\n",
        "            for inner in range(n_inner_epochs):\n",
        "                opt.zero_grad()\n",
        "                loss_total, loss_rec, h = self.augmented_lagrangian_loss(X_batch)\n",
        "                loss_total.backward()\n",
        "                opt.step()\n",
        "                self.apply_domain_mask_and_no_loops()\n",
        "            with torch.no_grad():\n",
        "                _, _, h = self.augmented_lagrangian_loss(X_batch)\n",
        "            self.update_dual(h)\n",
        "        return self.W.detach(), self.model\n",
        "    def get_binarized_adjacency(self, threshold=0.3):\n",
        "        return binarize_adjacency(self.W, threshold=threshold)\n",
        "\n",
        "def run_dagboost_method(X_data, threshold=0.3, max_iter=3, max_num_weak_learners=2,\n",
        "                        hidden_dim=4, lambda_h=5.0, verbose=False):\n",
        "    device = torch.device(\"cpu\")\n",
        "    d = X_data.shape[1]\n",
        "    adj_mask = np.ones((d, d), dtype=np.float32)\n",
        "    np.fill_diagonal(adj_mask, 0)\n",
        "    trainer = DAGBoostingTrainer(d, adjacency_mask=adj_mask, lr_W=0.01, lambda_h=lambda_h,\n",
        "                                 alpha_init=0.0, max_iter=max_iter,\n",
        "                                 max_num_weak_learners=max_num_weak_learners,\n",
        "                                 hidden_dim=hidden_dim, tol=1e-5, patience=2,\n",
        "                                 device=device)\n",
        "    trainer.train(X_data, batch_size=128, n_inner_epochs=10, fit_new_learner_epochs=5, verbose=verbose)\n",
        "    # Return the continuous W (for total effect estimation)\n",
        "    return trainer.W.detach().cpu().numpy(), trainer\n",
        "\n",
        "def flar(X_data, threshold=0.3, max_iter=3, max_num_weak_learners=2, hidden_dim=4,\n",
        "         lambda_h=5.0, verbose=False):\n",
        "    return run_dagboost_method(X_data, threshold=threshold, max_iter=max_iter,\n",
        "                               max_num_weak_learners=max_num_weak_learners,\n",
        "                               hidden_dim=hidden_dim, lambda_h=lambda_h, verbose=verbose)\n",
        "\n",
        "# -------------------------------\n",
        "# 4. Main Comprehensive Experiment: Normalized vs. Unnormalized Data\n",
        "# -------------------------------\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Set to True to execute the experiment.\n",
        "    run_cell = True\n",
        "    if not run_cell:\n",
        "        raise Exception(\"Execution halted.\")\n",
        "\n",
        "    # Define experimental configurations:\n",
        "    node_settings = [20, 50, 100, 500, 1000]  # number of nodes\n",
        "    dag_types = ['ER', 'SF']       # Erdős–Rényi and Scale‑Free\n",
        "    sem_types = ['linear']\n",
        "    noise_types = ['gaussian', 'exponential', 'laplace']\n",
        "    n_samples = 500                # number of samples per experiment\n",
        "    weight_scale = 1.0\n",
        "\n",
        "    # Prepare a list to hold experiment results\n",
        "    results = []\n",
        "\n",
        "    # Loop over configurations and for each run with raw and normalized data\n",
        "    for n_vars in node_settings:\n",
        "        # Set edge probability for ER: expected avg degree ~2\n",
        "        edge_prob = 2.0 / (n_vars - 1)\n",
        "        for dag in dag_types:\n",
        "            if dag == 'ER':\n",
        "                true_adj = generate_erdos_renyi_dag(n_vars, edge_prob)\n",
        "            elif dag == 'SF':\n",
        "                true_adj = generate_scale_free_dag(n_vars)\n",
        "            else:\n",
        "                continue\n",
        "            for sem in sem_types:\n",
        "                for noise in noise_types:\n",
        "                    config_str = f\"n_vars={n_vars}, DAG={dag}, SEM={sem}, Noise={noise}\"\n",
        "                    print(\"\\n[Config]\", config_str)\n",
        "                    # Simulate data using the appropriate SEM simulation function\n",
        "                    if sem == 'linear':\n",
        "                        X_data_raw, W_true = simulate_sem_with_weights(true_adj, n_samples,\n",
        "                                                                       noise_type=noise,\n",
        "                                                                       weight_scale=weight_scale,\n",
        "                                                                       random_state=42)\n",
        "                    else:\n",
        "                        X_data_raw, W_true = simulate_non_linear_sem_with_weights(true_adj, n_samples,\n",
        "                                                                                  noise_type=noise,\n",
        "                                                                                  weight_scale=weight_scale,\n",
        "                                                                                  random_state=42,\n",
        "                                                                                  non_linear_fn=np.tanh)\n",
        "                    # Create normalized data (standardization: zero mean, unit std)\n",
        "                    X_data_norm = (X_data_raw - X_data_raw.mean(axis=0)) / (X_data_raw.std(axis=0) + 1e-8)\n",
        "\n",
        "                    for norm_status, X_data in [('Normalized', X_data_norm)]:\n",
        "                        # Run DAG‑Boosting (FLAR)\n",
        "                        start_time = time.time()\n",
        "                        W_learned, _ = flar(X_data, threshold=0.3, max_iter=3,\n",
        "                                            max_num_weak_learners=2, hidden_dim=4,\n",
        "                                            lambda_h=5.0, verbose=False)\n",
        "                        runtime = time.time() - start_time\n",
        "\n",
        "                        # Compute total effect matrix and extract ATE (treatment = node 0, outcome = node n_vars-1)\n",
        "                        T_est = compute_total_effect_matrix(W_learned)\n",
        "                        ate_est = T_est[0, n_vars - 1]\n",
        "\n",
        "                        # Recover binary estimated graph using simple thresholding\n",
        "                        est_adj = threshold_W(W_learned, thr=0.3)\n",
        "\n",
        "                        # Compute evaluation metrics\n",
        "                        shd_val = shd(true_adj, est_adj)\n",
        "                        sid_val = sid(true_adj, est_adj)\n",
        "                        ate_rmse_val = rmse_ate(W_true, est_adj.astype(float))\n",
        "\n",
        "                        # Record results\n",
        "                        results.append({\n",
        "                            'n_vars': n_vars,\n",
        "                            'DAG_type': dag,\n",
        "                            'SEM_type': sem,\n",
        "                            'Noise': noise,\n",
        "                            'Runtime_sec': runtime,\n",
        "                            'SHD': shd_val,\n",
        "                            'SID': sid_val,\n",
        "                            'ATE_RMSE': ate_rmse_val,\n",
        "                        })\n",
        "\n",
        "    # Compile results into a DataFrame and display\n",
        "    df_results = pd.DataFrame(results)\n",
        "    print(\"\\n===== Comprehensive Experiment Results =====\")\n",
        "    print(df_results)\n",
        "\n",
        "    # Save the results DataFrame as CSV and JSON files\n",
        "    output_csv_filename = \"normalization_experiments_results.csv\"\n",
        "    output_json_filename = \"normalization_experiments_results.json\"\n",
        "\n",
        "    # Save to CSV (with headers, no index)\n",
        "    df_results.to_csv(output_csv_filename, index=False)\n",
        "    print(f\"Results saved to {output_csv_filename}\")\n",
        "\n",
        "    # Save to JSON in records format (one JSON object per line)\n",
        "    df_results.to_json(output_json_filename, orient=\"records\", lines=True)\n",
        "    print(f\"Results saved to {output_json_filename}\")"
      ],
      "metadata": {
        "id": "I60AbM5mmnWm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}