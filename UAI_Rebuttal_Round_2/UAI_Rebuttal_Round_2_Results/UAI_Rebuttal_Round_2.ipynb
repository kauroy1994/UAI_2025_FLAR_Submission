{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEVJSyAbgnpA"
      },
      "source": [
        "# **Package Installations**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2TPf8mdgr6X",
        "outputId": "c7715c31-0d6d-4aa9-8b17-1797c5ef75ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: cdt in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (0.6.0)\n",
            "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from cdt) (2.2.4)\n",
            "Requirement already satisfied: scipy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from cdt) (1.15.2)\n",
            "Requirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from cdt) (1.6.1)\n",
            "Requirement already satisfied: joblib in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from cdt) (1.4.2)\n",
            "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from cdt) (2.2.3)\n",
            "Requirement already satisfied: statsmodels in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from cdt) (0.14.4)\n",
            "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from cdt) (3.4.2)\n",
            "Requirement already satisfied: skrebate in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from cdt) (0.62)\n",
            "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from cdt) (4.67.1)\n",
            "Requirement already satisfied: GPUtil in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from cdt) (1.4.0)\n",
            "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from cdt) (2.32.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas->cdt) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas->cdt) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas->cdt) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->cdt) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->cdt) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->cdt) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->cdt) (2024.12.14)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scikit-learn->cdt) (3.6.0)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from statsmodels->cdt) (1.0.1)\n",
            "Requirement already satisfied: packaging>=21.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from statsmodels->cdt) (24.2)\n",
            "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->cdt) (1.17.0)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
            "Requirement already satisfied: torch in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.6.0)\n",
            "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: sympy==1.13.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from jinja2->torch) (3.0.2)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
            "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (3.4.2)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
            "Requirement already satisfied: causal-learn in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (0.1.4.1)\n",
            "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from causal-learn) (2.2.4)\n",
            "Requirement already satisfied: scipy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from causal-learn) (1.15.2)\n",
            "Requirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from causal-learn) (1.6.1)\n",
            "Requirement already satisfied: graphviz in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from causal-learn) (0.20.3)\n",
            "Requirement already satisfied: statsmodels in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from causal-learn) (0.14.4)\n",
            "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from causal-learn) (2.2.3)\n",
            "Requirement already satisfied: matplotlib in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from causal-learn) (3.10.1)\n",
            "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from causal-learn) (3.4.2)\n",
            "Requirement already satisfied: pydot in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from causal-learn) (3.0.4)\n",
            "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from causal-learn) (4.67.1)\n",
            "Requirement already satisfied: momentchi2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from causal-learn) (0.1.8)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib->causal-learn) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib->causal-learn) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib->causal-learn) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib->causal-learn) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib->causal-learn) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib->causal-learn) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib->causal-learn) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib->causal-learn) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas->causal-learn) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas->causal-learn) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scikit-learn->causal-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scikit-learn->causal-learn) (3.6.0)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from statsmodels->causal-learn) (1.0.1)\n",
            "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib->causal-learn) (1.17.0)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
            "Requirement already satisfied: statsmodels in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (0.14.4)\n",
            "Requirement already satisfied: numpy<3,>=1.22.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from statsmodels) (2.2.4)\n",
            "Requirement already satisfied: scipy!=1.9.2,>=1.8 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from statsmodels) (1.15.2)\n",
            "Requirement already satisfied: pandas!=2.1.0,>=1.4 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from statsmodels) (2.2.3)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from statsmodels) (1.0.1)\n",
            "Requirement already satisfied: packaging>=21.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from statsmodels) (24.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas!=2.1.0,>=1.4->statsmodels) (1.17.0)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
            "Collecting xges\n",
            "  Downloading xges-0.1.6-py3-none-any.whl (26 kB)\n",
            "Collecting numba<0.60.0,>=0.59.1\n",
            "  Downloading numba-0.59.1-cp311-cp311-macosx_11_0_arm64.whl (2.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
            "\u001b[?25hCollecting numpy<2.0.0,>=1.26.4\n",
            "  Downloading numpy-1.26.4-cp311-cp311-macosx_11_0_arm64.whl (14.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting sortedcontainers<3.0.0,>=2.4.0\n",
            "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
            "Collecting llvmlite<0.43,>=0.42.0dev0\n",
            "  Downloading llvmlite-0.42.0-cp311-cp311-macosx_11_0_arm64.whl (28.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28.8/28.8 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sortedcontainers, numpy, llvmlite, numba, xges\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.2.4\n",
            "    Uninstalling numpy-2.2.4:\n",
            "      Successfully uninstalled numpy-2.2.4\n",
            "Successfully installed llvmlite-0.42.0 numba-0.59.1 numpy-1.26.4 sortedcontainers-2.4.0 xges-0.1.6\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
            "Requirement already satisfied: numba in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (0.59.1)\n",
            "Requirement already satisfied: llvmlite<0.43,>=0.42.0dev0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from numba) (0.42.0)\n",
            "Requirement already satisfied: numpy<1.27,>=1.22 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from numba) (1.26.4)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip3 install cdt\n",
        "!pip3 install torch\n",
        "!pip3 install networkx\n",
        "!pip3 install causal-learn\n",
        "!pip3 install statsmodels\n",
        "!pip3 install xges\n",
        "!pip3 install numba"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSzYhUuF9XKo"
      },
      "source": [
        "# **Helper Functions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "iZyhh-pC9ZPF"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python\n",
        "\"\"\"\n",
        "Complete Script to Compare PC and FLAR\n",
        "across metrics (SID, SHD, ATE_RMSE, runtime) for variable counts 10, 50, and 100.\n",
        "Evaluations are done for:\n",
        "  • DAG types: Erdős–Rényi (ER) and Scale‑Free (SF)\n",
        "  • SEM types: linear and non‑linear\n",
        "  • Noise types: gaussian, exponential, and laplace\n",
        "\n",
        "Make sure to install dependencies:\n",
        "    pip install numpy pandas torch networkx causal-learn statsmodels\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import networkx as nx\n",
        "import time\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 1. DAG Generation and Data Simulation Functions\n",
        "# ------------------------------------------------\n",
        "\n",
        "def generate_erdos_renyi_dag(num_nodes, edge_prob):\n",
        "    \"\"\"\n",
        "    Generate an Erdős–Rényi random DAG with given edge probability.\n",
        "    \"\"\"\n",
        "    perm = np.random.permutation(num_nodes)\n",
        "    adjacency_matrix = np.zeros((num_nodes, num_nodes), dtype=int)\n",
        "    for i in range(num_nodes):\n",
        "        for j in range(i+1, num_nodes):\n",
        "            if np.random.rand() < edge_prob:\n",
        "                adjacency_matrix[perm[i], perm[j]] = 1\n",
        "    return adjacency_matrix\n",
        "\n",
        "def remove_all_cycles(G):\n",
        "    \"\"\"\n",
        "    Remove edges from cycles until the graph is a DAG.\n",
        "    Works in-place on a networkx DiGraph.\n",
        "    \"\"\"\n",
        "    while True:\n",
        "        try:\n",
        "            cycle_edges = nx.find_cycle(G, orientation='original')\n",
        "            for (u, v, _) in cycle_edges:\n",
        "                G.remove_edge(u, v)\n",
        "        except nx.NetworkXNoCycle:\n",
        "            break\n",
        "    return G\n",
        "\n",
        "def generate_scale_free_dag(num_nodes):\n",
        "    \"\"\"\n",
        "    Generate a scale-free DAG: first generate a scale-free network and then remove cycles.\n",
        "    \"\"\"\n",
        "    G = nx.scale_free_graph(num_nodes, seed=None)\n",
        "    G_simple = nx.DiGraph(G)  # convert to simple directed graph\n",
        "    G_simple.remove_edges_from(nx.selfloop_edges(G_simple))\n",
        "    G_dag = remove_all_cycles(G_simple.copy())\n",
        "    adj_matrix = nx.to_numpy_array(G_dag, dtype=int)\n",
        "    np.fill_diagonal(adj_matrix, 0)\n",
        "    return adj_matrix\n",
        "\n",
        "def simulate_sem_with_weights(adjacency_matrix, n_samples, noise_type='gaussian',\n",
        "                              weight_scale=1.0, random_state=None):\n",
        "    \"\"\"\n",
        "    Simulate data from a linear SEM:\n",
        "         X_j = sum_{i in Pa(j)} W_{i,j} * X_i + noise_j\n",
        "    Returns data X and the true weight matrix W.\n",
        "    \"\"\"\n",
        "    if random_state is not None:\n",
        "        np.random.seed(random_state)\n",
        "\n",
        "    num_nodes = adjacency_matrix.shape[0]\n",
        "    G = nx.DiGraph(adjacency_matrix)\n",
        "    topo_order = list(nx.topological_sort(G))\n",
        "\n",
        "    # Generate weights on edges\n",
        "    W = np.zeros((num_nodes, num_nodes))\n",
        "    for i in range(num_nodes):\n",
        "        for j in range(num_nodes):\n",
        "            if adjacency_matrix[i, j] == 1:\n",
        "                W[i, j] = np.random.normal(loc=0.0, scale=weight_scale)\n",
        "\n",
        "    X = np.zeros((n_samples, num_nodes))\n",
        "    for s in range(n_samples):\n",
        "        for node in topo_order:\n",
        "            parents = np.where(adjacency_matrix[:, node] == 1)[0]\n",
        "            parents_sum = np.sum(W[parents, node] * X[s, parents])\n",
        "            if noise_type.lower() == 'gaussian':\n",
        "                noise = np.random.normal(loc=0.0, scale=1.0)\n",
        "            elif noise_type.lower() == 'laplace':\n",
        "                noise = np.random.laplace(loc=0.0, scale=1.0)\n",
        "            elif noise_type.lower() == 'exponential':\n",
        "                noise = np.random.exponential(scale=1.0)\n",
        "            else:\n",
        "                raise ValueError(\"noise_type must be 'gaussian', 'laplace', or 'exponential'.\")\n",
        "            X[s, node] = parents_sum + noise\n",
        "    return X, W\n",
        "\n",
        "def simulate_non_linear_sem_with_weights(adjacency_matrix, n_samples, noise_type='gaussian',\n",
        "                                           weight_scale=1.0, random_state=None, non_linear_fn=np.tanh):\n",
        "    \"\"\"\n",
        "    Simulate data from a non-linear SEM:\n",
        "         X_j = f( sum_{i in Pa(j)} W_{i,j} * X_i ) + noise_j\n",
        "    Returns data X and the true weight matrix W.\n",
        "    \"\"\"\n",
        "    if random_state is not None:\n",
        "        np.random.seed(random_state)\n",
        "\n",
        "    num_nodes = adjacency_matrix.shape[0]\n",
        "    G = nx.DiGraph(adjacency_matrix)\n",
        "    topo_order = list(nx.topological_sort(G))\n",
        "\n",
        "    # Generate weights on edges\n",
        "    W = np.zeros((num_nodes, num_nodes))\n",
        "    for i in range(num_nodes):\n",
        "        for j in range(num_nodes):\n",
        "            if adjacency_matrix[i, j] == 1:\n",
        "                W[i, j] = np.random.normal(loc=0.0, scale=weight_scale)\n",
        "\n",
        "    X = np.zeros((n_samples, num_nodes))\n",
        "    for s in range(n_samples):\n",
        "        for node in topo_order:\n",
        "            parents = np.where(adjacency_matrix[:, node] == 1)[0]\n",
        "            parents_sum = np.sum(W[parents, node] * X[s, parents])\n",
        "            non_linear_term = non_linear_fn(parents_sum)\n",
        "            if noise_type.lower() == 'gaussian':\n",
        "                noise = np.random.normal(loc=0.0, scale=1.0)\n",
        "            elif noise_type.lower() == 'laplace':\n",
        "                noise = np.random.laplace(loc=0.0, scale=1.0)\n",
        "            elif noise_type.lower() == 'exponential':\n",
        "                noise = np.random.exponential(scale=1.0)\n",
        "            else:\n",
        "                raise ValueError(\"noise_type must be 'gaussian', 'laplace', or 'exponential'.\")\n",
        "            X[s, node] = non_linear_term + noise\n",
        "    return X, W\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 2. Metrics: SHD, SID, and ATE_RMSE\n",
        "# ------------------------------------------------\n",
        "\n",
        "def shd(true_adj: np.ndarray, est_adj: np.ndarray) -> int:\n",
        "    \"\"\"Compute Structural Hamming Distance.\"\"\"\n",
        "    return int(np.sum(true_adj != est_adj))\n",
        "\n",
        "def _compute_ancestors(adj: np.ndarray):\n",
        "    \"\"\"Helper for SID calculation.\"\"\"\n",
        "    G = nx.DiGraph(adj)\n",
        "    d = adj.shape[0]\n",
        "    ancestors_list = []\n",
        "    for node in range(d):\n",
        "        ancestors_list.append(set(nx.ancestors(G, node)))\n",
        "    return ancestors_list\n",
        "\n",
        "def sid(true_adj: np.ndarray, est_adj: np.ndarray) -> int:\n",
        "    \"\"\"Compute Structural Intervention Distance (SID).\"\"\"\n",
        "    true_anc = _compute_ancestors(true_adj)\n",
        "    est_anc = _compute_ancestors(est_adj)\n",
        "    d = true_adj.shape[0]\n",
        "    score = 0\n",
        "    for j in range(d):\n",
        "        diff_1 = true_anc[j].difference(est_anc[j])\n",
        "        diff_2 = est_anc[j].difference(true_anc[j])\n",
        "        score += len(diff_1) + len(diff_2)\n",
        "    return score\n",
        "\n",
        "def compute_total_effect_matrix(W: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Compute total effect matrix T = (I - W)^{-1} - I.\"\"\"\n",
        "    d = W.shape[0]\n",
        "    I = np.eye(d)\n",
        "    try:\n",
        "        inv = np.linalg.inv(I - W)\n",
        "    except np.linalg.LinAlgError:\n",
        "        return np.zeros((d, d))\n",
        "    return inv - I\n",
        "\n",
        "def rmse_ate(W_true: np.ndarray, W_est: np.ndarray) -> float:\n",
        "    \"\"\"Compute RMSE of total causal effects (ATE RMSE).\"\"\"\n",
        "    T_true = compute_total_effect_matrix(W_true)\n",
        "    T_est  = compute_total_effect_matrix(W_est)\n",
        "    return np.sqrt(np.mean((T_true - T_est) ** 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqLfPFUM_14Q"
      },
      "source": [
        "# **FLAR**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "sB_6kZXl_3YU"
      },
      "outputs": [],
      "source": [
        "# ------------------------------------------------\n",
        "# 3. FLAR Implementation\n",
        "# ------------------------------------------------\n",
        "\n",
        "def squared_loss(x_true, x_pred):\n",
        "    return 0.5 * torch.mean((x_true - x_pred)**2)\n",
        "\n",
        "def dag_constraint(W):\n",
        "    d = W.shape[0]\n",
        "    WW = W * W\n",
        "    expm_WW = torch.matrix_exp(WW)\n",
        "    h = torch.trace(expm_WW) - d\n",
        "    return h\n",
        "\n",
        "def apply_mask(W, mask):\n",
        "    with torch.no_grad():\n",
        "        W *= mask\n",
        "        d = W.shape[0]\n",
        "        for i in range(d):\n",
        "            W[i, i] = 0.0\n",
        "\n",
        "def binarize_adjacency(W, threshold=0.3):\n",
        "    W_np = W.detach().cpu().numpy()\n",
        "    W_bin = (np.abs(W_np) > threshold).astype(float)\n",
        "    np.fill_diagonal(W_bin, 0.0)\n",
        "    return W_bin\n",
        "\n",
        "class WeakLearnerNN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=40):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class FunctionalBoostingModel(nn.Module):\n",
        "    def __init__(self, d, max_num_weak_learners=50, hidden_dim=40):\n",
        "        super().__init__()\n",
        "        self.d = d\n",
        "        self.max_num_weak_learners = max_num_weak_learners\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.learners_for_var = [[] for _ in range(d)]\n",
        "        self.weak_learners = nn.ModuleList()\n",
        "        self.current_counts = [0]*d\n",
        "\n",
        "    def forward(self, X, W):\n",
        "        N, d = X.shape\n",
        "        device = X.device\n",
        "        Xhat = []\n",
        "        for i in range(d):\n",
        "            mask_row = W[i, :]\n",
        "            masked_input = X * mask_row\n",
        "            pred_i = torch.zeros((N, 1), dtype=X.dtype, device=device)\n",
        "            for learner in self.learners_for_var[i]:\n",
        "                pred_i += learner(masked_input)\n",
        "            Xhat.append(pred_i)\n",
        "        return torch.cat(Xhat, dim=1)\n",
        "\n",
        "    def add_weak_learner(self, i):\n",
        "        wl = WeakLearnerNN(input_dim=self.d, hidden_dim=self.hidden_dim)\n",
        "        self.weak_learners.append(wl)\n",
        "        self.learners_for_var[i].append(wl)\n",
        "        self.current_counts[i] += 1\n",
        "\n",
        "    def fit_new_weak_learner(self, i, X, residual_i, W, n_epochs=5, lr=0.01, verbose=False):\n",
        "        self.add_weak_learner(i)\n",
        "        wl = self.learners_for_var[i][-1]\n",
        "        optimizer = optim.Adam(wl.parameters(), lr=lr)\n",
        "        for epoch in range(n_epochs):\n",
        "            optimizer.zero_grad()\n",
        "            mask_row = W[i, :]\n",
        "            masked_input = X * mask_row\n",
        "            pred = wl(masked_input)\n",
        "            loss = torch.mean((pred - residual_i)**2)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            if verbose:\n",
        "                print(f\"    [WL-Fit Var {i} Ep {epoch}] Loss={loss.item():.6f}\")\n",
        "\n",
        "class DAGBoostingTrainer:\n",
        "    def __init__(\n",
        "        self,\n",
        "        d,\n",
        "        adjacency_mask=None,\n",
        "        lr_W=0.01,\n",
        "        lambda_h=5.0,\n",
        "        alpha_init=0.0,\n",
        "        max_iter=5,\n",
        "        max_num_weak_learners=50,\n",
        "        hidden_dim=40,\n",
        "        tol=1e-4,\n",
        "        patience=2,\n",
        "        device=torch.device(\"cpu\")\n",
        "    ):\n",
        "        self.d = d\n",
        "        self.model = FunctionalBoostingModel(\n",
        "            d=d,\n",
        "            max_num_weak_learners=max_num_weak_learners,\n",
        "            hidden_dim=hidden_dim\n",
        "        ).to(device)\n",
        "        # Initialize real-valued adjacency matrix W\n",
        "        W_init = 0.01 * torch.randn(d, d, device=device)\n",
        "        for i in range(d):\n",
        "            W_init[i, i] = 0.0\n",
        "        self.W = nn.Parameter(W_init)\n",
        "\n",
        "        if adjacency_mask is None:\n",
        "            adjacency_mask = np.ones((d, d), dtype=np.float32)\n",
        "            np.fill_diagonal(adjacency_mask, 0.)\n",
        "        self.adjacency_mask = torch.tensor(adjacency_mask, dtype=torch.float32, device=device)\n",
        "\n",
        "        self.lambda_h = lambda_h\n",
        "        self.alpha = alpha_init\n",
        "        self.lr_W = lr_W\n",
        "        self.max_iter = max_iter\n",
        "        self.tol = tol\n",
        "        self.patience = patience\n",
        "        self.device = device\n",
        "        self.best_loss = float('inf')\n",
        "        self.no_improv_steps = 0\n",
        "        self.stop_early = False\n",
        "\n",
        "    def parameters(self):\n",
        "        return list(self.model.parameters()) + [self.W]\n",
        "\n",
        "    def apply_domain_mask_and_no_loops(self):\n",
        "        apply_mask(self.W, self.adjacency_mask)\n",
        "\n",
        "    def augmented_lagrangian_loss(self, X):\n",
        "        Xhat = self.model(X, self.W)\n",
        "        recon = squared_loss(X, Xhat)\n",
        "        h_val = dag_constraint(self.W)\n",
        "        aug = self.alpha * h_val + 0.5 * self.lambda_h * (h_val ** 2)\n",
        "        return recon + aug, recon, h_val\n",
        "\n",
        "    def update_dual(self, h_val):\n",
        "        self.alpha = self.alpha + self.lambda_h * h_val.item()\n",
        "\n",
        "    def train(self, X, batch_size=512, n_inner_epochs=10, fit_new_learner_epochs=5, verbose=True):\n",
        "        if not isinstance(X, torch.Tensor):\n",
        "            X = torch.tensor(X, dtype=torch.float32, device=self.device)\n",
        "        else:\n",
        "            X = X.to(self.device)\n",
        "\n",
        "        N_full = X.shape[0]\n",
        "\n",
        "        for outer_iter in range(self.max_iter):\n",
        "            if verbose:\n",
        "                print(f\"\\n===== Outer Iteration {outer_iter+1}/{self.max_iter} =====\")\n",
        "            indices = np.random.permutation(N_full)\n",
        "            subset_idx = indices[:batch_size]\n",
        "            X_sub = X[subset_idx]\n",
        "\n",
        "            with torch.no_grad():\n",
        "                Xhat_sub = self.model(X_sub, self.W)\n",
        "                residuals_sub = X_sub - Xhat_sub\n",
        "                mse_val = torch.mean((residuals_sub)**2).item()\n",
        "            if verbose:\n",
        "                print(f\"  Sub-batch size={batch_size}, MSE before new learners: {mse_val:.6f}\")\n",
        "\n",
        "            d = X_sub.shape[1]\n",
        "            for i in range(d):\n",
        "                if self.model.current_counts[i] < self.model.max_num_weak_learners:\n",
        "                    residual_i_sub = residuals_sub[:, i:i+1]\n",
        "                    self.model.fit_new_weak_learner(i=i, X=X_sub, residual_i=residual_i_sub,\n",
        "                                                     W=self.W, n_epochs=fit_new_learner_epochs,\n",
        "                                                     lr=0.01, verbose=False)\n",
        "\n",
        "            # Update W via inner epochs\n",
        "            opt = optim.Adam(self.parameters(), lr=self.lr_W)\n",
        "            for epoch in range(n_inner_epochs):\n",
        "                opt.zero_grad()\n",
        "                loss_total, loss_recon, h_val = self.augmented_lagrangian_loss(X_sub)\n",
        "                loss_total.backward()\n",
        "                opt.step()\n",
        "                self.apply_domain_mask_and_no_loops()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                _, _, h_val = self.augmented_lagrangian_loss(X_sub)\n",
        "            self.update_dual(h_val)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                Xhat_sub = self.model(X_sub, self.W)\n",
        "                mse_val = torch.mean((X_sub - Xhat_sub)**2).item()\n",
        "                h_now = dag_constraint(self.W).item()\n",
        "            if verbose:\n",
        "                print(f\"  [Iteration {outer_iter+1}] MSE={mse_val:.6f}, h(W)={h_now:.6f}, alpha={self.alpha:.3f}\")\n",
        "            if mse_val < self.best_loss - self.tol:\n",
        "                self.best_loss = mse_val\n",
        "                self.no_improv_steps = 0\n",
        "            else:\n",
        "                self.no_improv_steps += 1\n",
        "            if self.no_improv_steps >= self.patience:\n",
        "                if verbose:\n",
        "                    print(\"No improvement; early stopping.\")\n",
        "                self.stop_early = True\n",
        "                break\n",
        "        return self.W.detach(), self.model\n",
        "\n",
        "    def get_binarized_adjacency(self, threshold=0.3):\n",
        "        return binarize_adjacency(self.W, threshold=threshold)\n",
        "\n",
        "def run_dagboost_method(X_data, threshold=0.3, max_iter=5, max_num_weak_learners=50,\n",
        "                        hidden_dim=40, lambda_h=5.0, verbose=False):\n",
        "    \"\"\"\n",
        "    Run the DAG‑Boosting (FLAR) method and return:\n",
        "       - final binary estimated adjacency matrix, and\n",
        "       - the trainer (for further access to model/W)\n",
        "    \"\"\"\n",
        "    device = torch.device(\"cpu\")\n",
        "    d = X_data.shape[1]\n",
        "    adjacency_mask = np.ones((d, d), dtype=np.float32)\n",
        "    np.fill_diagonal(adjacency_mask, 0.0)\n",
        "    trainer = DAGBoostingTrainer(\n",
        "        d=d,\n",
        "        adjacency_mask=adjacency_mask,\n",
        "        lr_W=0.01,\n",
        "        lambda_h=lambda_h,\n",
        "        alpha_init=0.0,\n",
        "        max_iter=max_iter,\n",
        "        max_num_weak_learners=max_num_weak_learners,\n",
        "        hidden_dim=hidden_dim,\n",
        "        tol=1e-5,\n",
        "        patience=2,\n",
        "        device=device\n",
        "    )\n",
        "    trainer.train(X_data, batch_size=128, n_inner_epochs=10, fit_new_learner_epochs=5, verbose=verbose)\n",
        "    W_bin = trainer.get_binarized_adjacency(threshold=threshold)\n",
        "    # Remove any residual cycles\n",
        "    G_nx = nx.DiGraph(W_bin)\n",
        "    remove_all_cycles(G_nx)\n",
        "    final_adj = nx.to_numpy_array(G_nx, dtype=int)\n",
        "    return final_adj, trainer\n",
        "\n",
        "def flar(X_data, threshold=0.3, max_iter=5, max_num_weak_learners=50, hidden_dim=40,\n",
        "         lambda_h=5.0, verbose=False):\n",
        "    \"\"\"\n",
        "    Wrapper for running DAG‑Boosting.\n",
        "    \"\"\"\n",
        "    return run_dagboost_method(X_data, threshold=threshold, max_iter=max_iter,\n",
        "                               max_num_weak_learners=max_num_weak_learners,\n",
        "                               hidden_dim=hidden_dim, lambda_h=lambda_h, verbose=verbose)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DGNWu2ff5gW"
      },
      "source": [
        "# **Comparison Against PC**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "008b517751b249a3beef3d74cdf2e274",
            "4add9933905245ce8d73d9a89f566ae9",
            "30a04890926d46389e616baaaa09fa2e",
            "954ad3ad2f6042e68a991345af5ad30a",
            "349e14a5ca9e45c081293b22ca1ef84f",
            "4a87647b37144ad0954424a2dd782476",
            "acff3eace9e94ed8b170a510c901bc2e",
            "ac8b0a4a3c0047ed9af019cad8a06f3a",
            "07f7ab3a4b514832aa55a7c7000fd32b",
            "e6b05f760a8b40ff9b8d5afdb1b02c17",
            "83d81d346c7f44b690d1463bb1fdd3be"
          ]
        },
        "id": "1IqD0DRigFGJ",
        "outputId": "788871f8-8ba0-4d98-c2a2-07c3926db7c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Config] n_vars=100, DAG=ER, SEM=linear, Noise=gaussian\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "008b517751b249a3beef3d74cdf2e274",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 4. PC Method (using causal‑learn)\n",
        "# ------------------------------------------------\n",
        "\n",
        "def run_pc_method(X_data):\n",
        "    \"\"\"\n",
        "    Run the PC algorithm from causal‑learn on data X_data.\n",
        "    Based on the usage in the causal-learn docs, PC is invoked as:\n",
        "         from causallearn.search.ConstraintBased.PC import pc\n",
        "         result = pc(X_data, alpha=0.05)\n",
        "    This function converts the resulting graph into a binary adjacency matrix,\n",
        "    where an entry of 1 in est_adj[i,j] indicates an edge i -> j.\n",
        "    \"\"\"\n",
        "    from causallearn.search.ConstraintBased.PC import pc\n",
        "    try:\n",
        "        # Run PC with a default significance level (alpha=0.05)\n",
        "        pc_result = pc(X_data, alpha=0.05)\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"PC algorithm failed with error: {e}\")\n",
        "\n",
        "    # Convert the result to an adjacency matrix.\n",
        "    # Here we assume pc_result.G.graph is a NumPy array encoding the graph.\n",
        "    pc_matrix = pc_result.G.graph\n",
        "    d = pc_matrix.shape[0]\n",
        "    est_adj = np.zeros((d, d), dtype=int)\n",
        "    # For each pair, if pc_matrix[j,i]==1, then there is an edge i -> j.\n",
        "    for i in range(d):\n",
        "        for j in range(d):\n",
        "            if pc_matrix[j, i] == 1:\n",
        "                est_adj[i, j] = 1\n",
        "    # Remove cycles if any\n",
        "    G_nx = nx.DiGraph(est_adj)\n",
        "    remove_all_cycles(G_nx)\n",
        "    final_adj = nx.to_numpy_array(G_nx, dtype=int)\n",
        "    return final_adj\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 5. Main Comparison Loop: Testing Configurations\n",
        "# ------------------------------------------------\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    run_cell = True  # Set run_cell to True to execute\n",
        "    if not run_cell:\n",
        "        class StopExecutionWithMessage(Exception):\n",
        "            def __init__(self, message):\n",
        "                self.message = message\n",
        "            def _render_traceback_(self):\n",
        "                print(self.message)\n",
        "                return []\n",
        "        raise StopExecutionWithMessage(\"Execution halted.\")\n",
        "\n",
        "    # Experimental configurations\n",
        "    var_settings = [100, 500]       # Number of variables\n",
        "    dag_types = ['ER','SF']           # ER: Erdős–Rényi, SF: Scale-Free\n",
        "    sem_types = ['linear','non-linear']\n",
        "    noise_types = ['gaussian','exponential','laplace']  # Use \"laplace\" for Laplace noise\n",
        "    n_samples = 500                    # Number of samples per experiment\n",
        "    weight_scale = 1.0\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for n_vars in var_settings:\n",
        "        edge_prob = 2.0 / (n_vars - 1)  # Expected average degree ~2\n",
        "        for dag_type in dag_types:\n",
        "            # Generate the true DAG\n",
        "            if dag_type == 'ER':\n",
        "                true_adj = generate_erdos_renyi_dag(n_vars, edge_prob)\n",
        "            elif dag_type == 'SF':\n",
        "                true_adj = generate_scale_free_dag(n_vars)\n",
        "            else:\n",
        "                continue\n",
        "\n",
        "            for sem_type in sem_types:\n",
        "                for noise in noise_types:\n",
        "                    config_str = f\"n_vars={n_vars}, DAG={dag_type}, SEM={sem_type}, Noise={noise}\"\n",
        "                    print(\"\\n[Config] \" + config_str)\n",
        "                    # Simulate data and obtain true weight matrix\n",
        "                    if sem_type == 'linear':\n",
        "                        X_data, W_true = simulate_sem_with_weights(true_adj, n_samples,\n",
        "                                                                   noise_type=noise,\n",
        "                                                                   weight_scale=weight_scale)\n",
        "                    else:\n",
        "                        X_data, W_true = simulate_non_linear_sem_with_weights(true_adj, n_samples,\n",
        "                                                                              noise_type=noise,\n",
        "                                                                              weight_scale=weight_scale,\n",
        "                                                                              non_linear_fn=np.tanh)\n",
        "\n",
        "                    # Create normalized data (standardization: zero mean, unit std)\n",
        "                    X_data_norm = (X_data - X_data.mean(axis=0)) / (X_data.std(axis=0) + 1e-8)\n",
        "                    # ---------------------------\n",
        "                    # Run PC Method\n",
        "                    try:\n",
        "                        t0 = time.time()\n",
        "                        est_adj_pc = run_pc_method(X_data)\n",
        "                        runtime_pc = time.time() - t0\n",
        "                        shd_pc = shd(true_adj, est_adj_pc)\n",
        "                        sid_pc = sid(true_adj, est_adj_pc)\n",
        "                        ate_rmse_pc = rmse_ate(W_true, est_adj_pc.astype(float))\n",
        "                    except Exception as e:\n",
        "                        runtime_pc = np.nan\n",
        "                        shd_pc = np.nan\n",
        "                        sid_pc = np.nan\n",
        "                        ate_rmse_pc = np.nan\n",
        "                        print(\"  PC method failed with error:\", e)\n",
        "\n",
        "                    results.append({\n",
        "                        'Method': 'PC',\n",
        "                        'n_vars': n_vars,\n",
        "                        'DAG_type': dag_type,\n",
        "                        'SEM_type': sem_type,\n",
        "                        'Noise': noise,\n",
        "                        'Runtime_sec': runtime_pc,\n",
        "                        'SHD': shd_pc,\n",
        "                        'SID': sid_pc,\n",
        "                        'ATE_RMSE': ate_rmse_pc\n",
        "                    })\n",
        "\n",
        "                    # ---------------------------\n",
        "                    # Run DAG‑Boosting (FLAR) Method\n",
        "                    try:\n",
        "                        t0 = time.time()\n",
        "                        est_adj_dagboost, trainer = flar(X_data, threshold=0.3, verbose=False)\n",
        "                        runtime_dagboost = time.time() - t0\n",
        "                        shd_dagboost = shd(true_adj, est_adj_dagboost)\n",
        "                        sid_dagboost = sid(true_adj, est_adj_dagboost)\n",
        "                        ate_rmse_dagboost = rmse_ate(W_true, est_adj_dagboost.astype(float))\n",
        "                    except Exception as e:\n",
        "                        runtime_dagboost = np.nan\n",
        "                        shd_dagboost = np.nan\n",
        "                        sid_dagboost = np.nan\n",
        "                        ate_rmse_dagboost = np.nan\n",
        "                        print(\"  DAG‑Boosting method failed with error:\", e)\n",
        "\n",
        "                    results.append({\n",
        "                        'Method': 'DAGBoost',\n",
        "                        'n_vars': n_vars,\n",
        "                        'DAG_type': dag_type,\n",
        "                        'SEM_type': sem_type,\n",
        "                        'Noise': noise,\n",
        "                        'Runtime_sec': runtime_dagboost,\n",
        "                        'SHD': shd_dagboost,\n",
        "                        'SID': sid_dagboost,\n",
        "                        'ATE_RMSE': ate_rmse_dagboost\n",
        "                    })\n",
        "\n",
        "    # Compile and print results as a DataFrame\n",
        "    df_results = pd.DataFrame(results)\n",
        "    print(\"\\n===== Comparison Results =====\")\n",
        "    print(df_results)\n",
        "\n",
        "    # Save the results DataFrame as CSV and JSON files\n",
        "    output_csv_filename = \"PC_results.csv\"\n",
        "    output_json_filename = \"PC_results.json\"\n",
        "\n",
        "    # Save to CSV (with headers, no index)\n",
        "    df_results.to_csv(output_csv_filename, index=False)\n",
        "    print(f\"Results saved to {output_csv_filename}\")\n",
        "\n",
        "    # Save to JSON in records format (one JSON object per line)\n",
        "    df_results.to_json(output_json_filename, orient=\"records\", lines=True)\n",
        "    print(f\"Results saved to {output_json_filename}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6NJFfJzl2gj"
      },
      "source": [
        "# **Comparison with fGES**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "lSa91s7Nl4oN",
        "outputId": "0cf0cfef-7fcb-4d67-f669-b2327cbc06ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Config] n_vars=500, DAG=ER, SEM=linear, Noise=gaussian\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "XGES: Package `numba` not found. Falling back to the slower BICScorer implementation. Install `numba` for better performance with `pip install numba`.\n",
            "XGES: Final score: -130135.16249911231\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===== Comparison Results =====\n",
            "     Method  n_vars DAG_type SEM_type     Noise  Runtime_sec  SHD   SID  \\\n",
            "0       GES     500       ER   linear  gaussian  1738.693927  287  3324   \n",
            "1  DAGBoost     500       ER   linear  gaussian    25.805694  509  1000   \n",
            "\n",
            "   ATE_RMSE  \n",
            "0  0.139818  \n",
            "1  0.053367  \n",
            "Results saved to GES_500_results.csv\n",
            "Results saved to GES_500_results.json\n"
          ]
        }
      ],
      "source": [
        "def run_ges_method(X_data):\n",
        "    \"\"\"\n",
        "    Run the XGES algorithm on data X_data.\n",
        "\n",
        "    Returns a binary adjacency matrix such that est_adj[i,j]=1 indicates edge i → j.\n",
        "    \"\"\"\n",
        "    from xges import XGES\n",
        "    import numpy as np\n",
        "    import networkx as nx\n",
        "\n",
        "    try:\n",
        "        xges = XGES()\n",
        "        pdag = xges.fit(X_data)\n",
        "        dag = pdag.get_dag_extension()\n",
        "        est_adj = dag.to_adjacency_matrix()\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"XGES algorithm failed with error: {e}\")\n",
        "\n",
        "    # Remove any residual cycles.\n",
        "    G_nx = nx.DiGraph(est_adj)\n",
        "    remove_all_cycles(G_nx)\n",
        "    final_adj = nx.to_numpy_array(G_nx, dtype=int)\n",
        "    return final_adj\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    run_cell = True  # Set run_cell to True to execute\n",
        "    if not run_cell:\n",
        "        class StopExecutionWithMessage(Exception):\n",
        "            def __init__(self, message):\n",
        "                self.message = message\n",
        "            def _render_traceback_(self):\n",
        "                print(self.message)\n",
        "                return []\n",
        "        raise StopExecutionWithMessage(\"Execution halted.\")\n",
        "\n",
        "    # Experimental configurations\n",
        "    var_settings = [500]       # Number of variables\n",
        "    dag_types = ['ER']#, 'SF']           # ER: Erdős–Rényi, SF: Scale‑Free\n",
        "    sem_types = ['linear']#, 'non-linear']\n",
        "    noise_types = ['gaussian']#, 'exponential', 'laplace']  # Use \"laplace\" for Laplace noise\n",
        "    n_samples = 500                    # Number of samples per experiment\n",
        "    weight_scale = 1.0\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for n_vars in var_settings:\n",
        "        # Set edge probability for an expected average degree of ~2\n",
        "        edge_prob = 2.0 / (n_vars - 1)\n",
        "        for dag_type in dag_types:\n",
        "            # Generate the true DAG\n",
        "            if dag_type == 'ER':\n",
        "                true_adj = generate_erdos_renyi_dag(n_vars, edge_prob)\n",
        "            elif dag_type == 'SF':\n",
        "                true_adj = generate_scale_free_dag(n_vars)\n",
        "            else:\n",
        "                continue\n",
        "\n",
        "            for sem_type in sem_types:\n",
        "                for noise in noise_types:\n",
        "                    config_str = f\"n_vars={n_vars}, DAG={dag_type}, SEM={sem_type}, Noise={noise}\"\n",
        "                    print(\"\\n[Config] \" + config_str)\n",
        "                    # Simulate data and obtain true weight matrix\n",
        "                    if sem_type == 'linear':\n",
        "                        X_data, W_true = simulate_sem_with_weights(true_adj, n_samples,\n",
        "                                                                   noise_type=noise,\n",
        "                                                                   weight_scale=weight_scale)\n",
        "                    else:  # non-linear\n",
        "                        X_data, W_true = simulate_non_linear_sem_with_weights(true_adj, n_samples,\n",
        "                                                                              noise_type=noise,\n",
        "                                                                              weight_scale=weight_scale,\n",
        "                                                                              non_linear_fn=np.tanh)\n",
        "                    # Create normalized data (standardization: zero mean, unit std)\n",
        "                    X_data_norm = (X_data - X_data.mean(axis=0)) / (X_data.std(axis=0) + 1e-8)\n",
        "                    # ---------------------------\n",
        "                    # Run GES Method\n",
        "                    try:\n",
        "                        t0 = time.time()\n",
        "                        est_adj_ges = run_ges_method(X_data)\n",
        "                        runtime_ges = time.time() - t0\n",
        "                        shd_ges = shd(true_adj, est_adj_ges)\n",
        "                        sid_ges = sid(true_adj, est_adj_ges)\n",
        "                        ate_rmse_ges = rmse_ate(W_true, est_adj_ges.astype(float))\n",
        "                    except Exception as e:\n",
        "                        runtime_ges = np.nan\n",
        "                        shd_ges = np.nan\n",
        "                        sid_ges = np.nan\n",
        "                        ate_rmse_ges = np.nan\n",
        "                        print(\"  GES method failed with error:\", e)\n",
        "\n",
        "                    results.append({\n",
        "                        'Method': 'GES',\n",
        "                        'n_vars': n_vars,\n",
        "                        'DAG_type': dag_type,\n",
        "                        'SEM_type': sem_type,\n",
        "                        'Noise': noise,\n",
        "                        'Runtime_sec': runtime_ges,\n",
        "                        'SHD': shd_ges,\n",
        "                        'SID': sid_ges,\n",
        "                        'ATE_RMSE': ate_rmse_ges\n",
        "                    })\n",
        "\n",
        "                    # ---------------------------\n",
        "                    # Run DAG‑Boosting (FLAR) Method\n",
        "                    try:\n",
        "                        t0 = time.time()\n",
        "                        est_adj_dagboost, trainer = flar(X_data, threshold=0.3, verbose=False)\n",
        "                        runtime_dagboost = time.time() - t0\n",
        "                        shd_dagboost = shd(true_adj, est_adj_dagboost)\n",
        "                        sid_dagboost = sid(true_adj, est_adj_dagboost)\n",
        "                        ate_rmse_dagboost = rmse_ate(W_true, est_adj_dagboost.astype(float))\n",
        "                    except Exception as e:\n",
        "                        runtime_dagboost = np.nan\n",
        "                        shd_dagboost = np.nan\n",
        "                        sid_dagboost = np.nan\n",
        "                        ate_rmse_dagboost = np.nan\n",
        "                        print(\"  DAG‑Boosting method failed with error:\", e)\n",
        "\n",
        "                    results.append({\n",
        "                        'Method': 'DAGBoost',\n",
        "                        'n_vars': n_vars,\n",
        "                        'DAG_type': dag_type,\n",
        "                        'SEM_type': sem_type,\n",
        "                        'Noise': noise,\n",
        "                        'Runtime_sec': runtime_dagboost,\n",
        "                        'SHD': shd_dagboost,\n",
        "                        'SID': sid_dagboost,\n",
        "                        'ATE_RMSE': ate_rmse_dagboost\n",
        "                    })\n",
        "\n",
        "    # Compile and print results as a DataFrame\n",
        "    df_results = pd.DataFrame(results)\n",
        "    print(\"\\n===== Comparison Results =====\")\n",
        "    print(df_results)\n",
        "\n",
        "    # Save the results DataFrame as CSV and JSON files\n",
        "    output_csv_filename = \"GES_500_results.csv\"\n",
        "    output_json_filename = \"GES_500_results.json\"\n",
        "\n",
        "    # Save to CSV (with headers, no index)\n",
        "    df_results.to_csv(output_csv_filename, index=False)\n",
        "    print(f\"Results saved to {output_csv_filename}\")\n",
        "\n",
        "    # Save to JSON in records format (one JSON object per line)\n",
        "    df_results.to_json(output_json_filename, orient=\"records\", lines=True)\n",
        "    print(f\"Results saved to {output_json_filename}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDRBdex_5KAc"
      },
      "source": [
        "# **Comparison with SDCD**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GYIEvlSP7vAa",
        "outputId": "742a3b51-09df-48f8-8d3c-6585af8686ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting sdcd\n",
            "  Downloading sdcd-0.1.4-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: networkx<4.0.0,>=3.2.1 in /usr/local/lib/python3.11/dist-packages (from sdcd) (3.4.2)\n",
            "Collecting numba<0.60.0,>=0.59.1 (from sdcd)\n",
            "  Downloading numba-0.59.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.7 kB)\n",
            "Collecting pandas<2.0.0,>=1.1.1 (from sdcd)\n",
            "  Downloading pandas-1.5.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.11.3 in /usr/local/lib/python3.11/dist-packages (from sdcd) (1.14.1)\n",
            "Requirement already satisfied: torch<3.0.0,>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from sdcd) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.11/dist-packages (from sdcd) (4.67.1)\n",
            "Collecting wandb<0.16.0,>=0.15.12 (from sdcd)\n",
            "  Downloading wandb-0.15.12-py3-none-any.whl.metadata (9.8 kB)\n",
            "Collecting llvmlite<0.43,>=0.42.0dev0 (from numba<0.60.0,>=0.59.1->sdcd)\n",
            "  Downloading llvmlite-0.42.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.8 kB)\n",
            "Collecting numpy<1.27,>=1.22 (from numba<0.60.0,>=0.59.1->sdcd)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.11/dist-packages (from pandas<2.0.0,>=1.1.1->sdcd) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<2.0.0,>=1.1.1->sdcd) (2025.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.1.0->sdcd) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.1.0->sdcd) (4.13.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.1.0->sdcd) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.1.0->sdcd) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.1.0->sdcd) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.1.0->sdcd) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.1.0->sdcd) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.1.0->sdcd) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.1.0->sdcd) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.1.0->sdcd) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.1.0->sdcd) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.1.0->sdcd) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.1.0->sdcd) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.1.0->sdcd) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.1.0->sdcd) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.1.0->sdcd) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.1.0->sdcd) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.1.0->sdcd) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.1.0->sdcd) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3.0.0,>=2.1.0->sdcd) (1.3.0)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb<0.16.0,>=0.15.12->sdcd) (8.1.8)\n",
            "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb<0.16.0,>=0.15.12->sdcd) (3.1.44)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb<0.16.0,>=0.15.12->sdcd) (2.32.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb<0.16.0,>=0.15.12->sdcd) (5.9.5)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb<0.16.0,>=0.15.12->sdcd) (2.25.1)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb<0.16.0,>=0.15.12->sdcd) (0.4.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from wandb<0.16.0,>=0.15.12->sdcd) (6.0.2)\n",
            "Collecting pathtools (from wandb<0.16.0,>=0.15.12->sdcd)\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb<0.16.0,>=0.15.12->sdcd) (1.3.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb<0.16.0,>=0.15.12->sdcd) (75.2.0)\n",
            "Collecting appdirs>=1.4.3 (from wandb<0.16.0,>=0.15.12->sdcd)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting protobuf!=4.21.0,<5,>=3.19.0 (from wandb<0.16.0,>=0.15.12->sdcd)\n",
            "  Downloading protobuf-4.25.6-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb<0.16.0,>=0.15.12->sdcd) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb<0.16.0,>=0.15.12->sdcd) (4.0.12)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb<0.16.0,>=0.15.12->sdcd) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb<0.16.0,>=0.15.12->sdcd) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb<0.16.0,>=0.15.12->sdcd) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb<0.16.0,>=0.15.12->sdcd) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3.0.0,>=2.1.0->sdcd) (3.0.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb<0.16.0,>=0.15.12->sdcd) (5.0.2)\n",
            "Downloading sdcd-0.1.4-py3-none-any.whl (69 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.6/69.6 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numba-0.59.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-1.5.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m98.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wandb-0.15.12-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m92.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading llvmlite-0.42.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (43.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 MB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m98.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-4.25.6-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8792 sha256=727e404f847813c6f320e882ed304d2a5b71fe5df547b859f1fb8ac70d133df7\n",
            "  Stored in directory: /root/.cache/pip/wheels/ea/b7/8b/84e94095ea418b9442f5abeba4ca7b0ad52d3fe7b69d6238a6\n",
            "Successfully built pathtools\n",
            "Installing collected packages: pathtools, appdirs, protobuf, numpy, llvmlite, pandas, numba, wandb, sdcd\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.4\n",
            "    Uninstalling protobuf-5.29.4:\n",
            "      Successfully uninstalled protobuf-5.29.4\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: llvmlite\n",
            "    Found existing installation: llvmlite 0.43.0\n",
            "    Uninstalling llvmlite-0.43.0:\n",
            "      Successfully uninstalled llvmlite-0.43.0\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "  Attempting uninstall: numba\n",
            "    Found existing installation: numba 0.60.0\n",
            "    Uninstalling numba-0.60.0:\n",
            "      Successfully uninstalled numba-0.60.0\n",
            "  Attempting uninstall: wandb\n",
            "    Found existing installation: wandb 0.19.9\n",
            "    Uninstalling wandb-0.19.9:\n",
            "      Successfully uninstalled wandb-0.19.9\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 1.5.3 which is incompatible.\n",
            "grpcio-status 1.71.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.6 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "mizani 0.13.2 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "dask-expr 1.1.21 requires pandas>=2, but you have pandas 1.5.3 which is incompatible.\n",
            "cudf-cu12 25.2.1 requires pandas<2.2.4dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "xarray 2025.1.2 requires pandas>=2.1, but you have pandas 1.5.3 which is incompatible.\n",
            "plotnine 0.14.5 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "dask-cudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "ydf 0.11.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 4.25.6 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed appdirs-1.4.4 llvmlite-0.42.0 numba-0.59.1 numpy-1.26.4 pandas-1.5.3 pathtools-0.1.2 protobuf-4.25.6 sdcd-0.1.4 wandb-0.15.12\n",
            "Collecting wandb==0.15.10\n",
            "  Downloading wandb-0.15.10-py3-none-any.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb==0.15.10) (8.1.8)\n",
            "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb==0.15.10) (3.1.44)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb==0.15.10) (2.32.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb==0.15.10) (5.9.5)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb==0.15.10) (2.25.1)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb==0.15.10) (0.4.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from wandb==0.15.10) (6.0.2)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.11/dist-packages (from wandb==0.15.10) (0.1.2)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb==0.15.10) (1.3.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb==0.15.10) (75.2.0)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.11/dist-packages (from wandb==0.15.10) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb==0.15.10) (4.25.6)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb==0.15.10) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb==0.15.10) (4.0.12)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb==0.15.10) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb==0.15.10) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb==0.15.10) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb==0.15.10) (2025.1.31)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb==0.15.10) (5.0.2)\n",
            "Downloading wandb-0.15.10-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: wandb\n",
            "  Attempting uninstall: wandb\n",
            "    Found existing installation: wandb 0.15.12\n",
            "    Uninstalling wandb-0.15.12:\n",
            "      Successfully uninstalled wandb-0.15.12\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sdcd 0.1.4 requires wandb<0.16.0,>=0.15.12, but you have wandb 0.15.10 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed wandb-0.15.10\n",
            "Collecting protobuf==3.20.3\n",
            "  Downloading protobuf-3.20.3-py2.py3-none-any.whl.metadata (720 bytes)\n",
            "Downloading protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: protobuf\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 4.25.6\n",
            "    Uninstalling protobuf-4.25.6:\n",
            "      Successfully uninstalled protobuf-4.25.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sdcd 0.1.4 requires wandb<0.16.0,>=0.15.12, but you have wandb 0.15.10 which is incompatible.\n",
            "grpcio-status 1.71.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.20.3 which is incompatible.\n",
            "ydf 0.11.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 3.20.3 which is incompatible.\n",
            "tensorflow-metadata 1.17.0 requires protobuf<6.0.0,>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed protobuf-3.20.3\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "e3746bcc5c24408d8bcf3f9b724a4f86",
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install sdcd\n",
        "!pip install wandb==0.15.10\n",
        "!pip install protobuf==3.20.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xq1QfnzB5Mfn"
      },
      "outputs": [],
      "source": [
        "from sdcd.models import SDCD\n",
        "from sdcd.utils import create_intervention_dataset\n",
        "\n",
        "def run_sdcd_method(n_vars):\n",
        "\n",
        "  # Simulate Data\n",
        "  from sdcd.simulated_data import random_model_gaussian_global_variance # For demonstration\n",
        "\n",
        "  n = 500\n",
        "  n_per_intervention = 50\n",
        "  d = n_vars\n",
        "  n_edges = int(d*(2.0 / (n_vars - 1)))\n",
        "\n",
        "  true_causal_model = random_model_gaussian_global_variance(\n",
        "      d,\n",
        "      n_edges,\n",
        "      dag_type=\"ER\",\n",
        "      scale=0.5,\n",
        "      hard=True,\n",
        "  )\n",
        "  X_df = true_causal_model.generate_dataframe_from_all_distributions(\n",
        "      n_samples_control=n,\n",
        "      n_samples_per_intervention=n_per_intervention,\n",
        "  )\n",
        "  X_df.iloc[:, :-1] = (X_df.iloc[:, :-1] - X_df.iloc[:, :-1].mean()) / X_df.iloc[\n",
        "      :, :-1\n",
        "  ].std() # Normalize the data\n",
        "\n",
        "  X_dataset = create_intervention_dataset(X_df, perturbation_colname=\"perturbation_label\")\n",
        "  model = SDCD()\n",
        "  model.train(X_dataset)\n",
        "  adj_matrix = model.get_adjacency_matrix(threshold=True)\n",
        "  return adj_matrix\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    run_cell = True  # Set this to True to execute the comparison\n",
        "    if not run_cell:\n",
        "        class StopExecution(Exception):\n",
        "            def __init__(self, msg):\n",
        "                self.msg = msg\n",
        "            def _render_traceback_(self):\n",
        "                print(self.msg)\n",
        "                return []\n",
        "        raise StopExecution(\"Execution halted.\")\n",
        "\n",
        "    # Experimental configurations\n",
        "    var_settings = [100,500]       # Number of variables\n",
        "    dag_types = ['ER', 'SF']           # DAG types: Erdős–Rényi, Scale‑Free\n",
        "    sem_types = ['linear', 'non-linear']\n",
        "    noise_types = ['gaussian', 'exponential', 'laplace']  # Use \"laplace\" for Laplace noise\n",
        "    n_samples = 500                    # Samples per experiment\n",
        "    weight_scale = 1.0\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for n_vars in var_settings:\n",
        "        edge_prob = 2.0 / (n_vars - 1)  # Expected average degree ~2\n",
        "        for dag_type in dag_types:\n",
        "            # Generate true DAG\n",
        "            if dag_type == 'ER':\n",
        "                true_adj = generate_erdos_renyi_dag(n_vars, edge_prob)\n",
        "            elif dag_type == 'SF':\n",
        "                true_adj = generate_scale_free_dag(n_vars)\n",
        "            else:\n",
        "                continue\n",
        "            for sem_type in sem_types:\n",
        "                for noise in noise_types:\n",
        "                    config_str = f\"n_vars={n_vars}, DAG={dag_type}, SEM={sem_type}, Noise={noise}\"\n",
        "                    print(\"\\n[Config] \" + config_str)\n",
        "                    # Simulate data and get true weight matrix\n",
        "                    if sem_type == 'linear':\n",
        "                        X_data, W_true = simulate_sem_with_weights(true_adj, n_samples,\n",
        "                                                                   noise_type=noise,\n",
        "                                                                   weight_scale=weight_scale)\n",
        "                    else:\n",
        "                        X_data, W_true = simulate_non_linear_sem_with_weights(true_adj, n_samples,\n",
        "                                                                              noise_type=noise,\n",
        "                                                                              weight_scale=weight_scale,\n",
        "                                                                              non_linear_fn=np.tanh)\n",
        "                    # Create normalized data (standardization: zero mean, unit std)\n",
        "                    X_data_norm = (X_data - X_data.mean(axis=0)) / (X_data.std(axis=0) + 1e-8)\n",
        "                    # ---------------------------\n",
        "                    # Run SDCD Method\n",
        "                    try:\n",
        "                        t0 = time.time()\n",
        "                        est_adj_sdcd = run_sdcd_method(n_vars)\n",
        "                        runtime_sdcd = time.time() - t0\n",
        "                        shd_sdcd = shd(true_adj, est_adj_sdcd)\n",
        "                        sid_sdcd = sid(true_adj, est_adj_sdcd)\n",
        "                        ate_rmse_sdcd = rmse_ate(W_true, est_adj_sdcd.astype(float))\n",
        "                    except Exception as e:\n",
        "                        runtime_sdcd = np.nan\n",
        "                        shd_sdcd = np.nan\n",
        "                        sid_sdcd = np.nan\n",
        "                        ate_rmse_sdcd = np.nan\n",
        "                        print(\"  SDCD method failed with error:\", e)\n",
        "\n",
        "                    results.append({\n",
        "                        'Method': 'SDCD',\n",
        "                        'n_vars': n_vars,\n",
        "                        'DAG_type': dag_type,\n",
        "                        'SEM_type': sem_type,\n",
        "                        'Noise': noise,\n",
        "                        'Runtime_sec': runtime_sdcd,\n",
        "                        'SHD': shd_sdcd,\n",
        "                        'SID': sid_sdcd,\n",
        "                        'ATE_RMSE': ate_rmse_sdcd\n",
        "                    })\n",
        "\n",
        "                    # ---------------------------\n",
        "                    # Run DAG‑Boosting (FLAR) Method\n",
        "                    try:\n",
        "                        t0 = time.time()\n",
        "                        est_adj_dagboost, trainer = (X_data, threshold=0.3, verbose=False)\n",
        "                        runtime_dagboost = time.time() - t0\n",
        "                        shd_dagboost = shd(true_adj, est_adj_dagboost)\n",
        "                        sid_dagboost = sid(true_adj, est_adj_dagboost)\n",
        "                        ate_rmse_dagboost = rmse_ate(W_true, est_adj_dagboost.astype(float))\n",
        "                    except Exception as e:\n",
        "                        runtime_dagboost = np.nan\n",
        "                        shd_dagboost = np.nan\n",
        "                        sid_dagboost = np.nan\n",
        "                        ate_rmse_dagboost = np.nan\n",
        "                        print(\"  DAG‑Boosting method failed with error:\", e)\n",
        "\n",
        "                    results.append({\n",
        "                        'Method': 'DAGBoost',\n",
        "                        'n_vars': n_vars,\n",
        "                        'DAG_type': dag_type,\n",
        "                        'SEM_type': sem_type,\n",
        "                        'Noise': noise,\n",
        "                        'Runtime_sec': runtime_dagboost,\n",
        "                        'SHD': shd_dagboost,\n",
        "                        'SID': sid_dagboost,\n",
        "                        'ATE_RMSE': ate_rmse_dagboost\n",
        "                    })\n",
        "\n",
        "    df_results = pd.DataFrame(results)\n",
        "    print(\"\\n===== Comparison Results =====\")\n",
        "    print(df_results)\n",
        "\n",
        "    # Save the results DataFrame as CSV and JSON files\n",
        "    output_csv_filename = \"SDCD_results.csv\"\n",
        "    output_json_filename = \"SDCD_results.json\"\n",
        "\n",
        "    # Save to CSV (with headers, no index)\n",
        "    df_results.to_csv(output_csv_filename, index=False)\n",
        "    print(f\"Results saved to {output_csv_filename}\")\n",
        "\n",
        "    # Save to JSON in records format (one JSON object per line)\n",
        "    df_results.to_json(output_json_filename, orient=\"records\", lines=True)\n",
        "    print(f\"Results saved to {output_json_filename}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhDl-mF7AYhN"
      },
      "source": [
        "# **Comparison with SAM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "j046C_TFAbpo",
        "outputId": "c632e68c-b894-4fc9-9c91-d95c6fb387fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Config] n_vars=50, DAG=ER, SEM=linear, Noise=gaussian\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|▏         | 58/4000 [00:15<17:36,  3.73it/s, disc=0.00417, gen=-1.1, regul_loss=47.5, tot=-7.26]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-2dd626c7d977>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     93\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m                         \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m                         \u001b[0mest_adj_sam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_sam_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnruns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m                         \u001b[0mruntime_sam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m                         \u001b[0mshd_sam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_adj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mest_adj_sam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-2dd626c7d977>\u001b[0m in \u001b[0;36mrun_sam_method\u001b[0;34m(X_data, nruns)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0msam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSAM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnruns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnruns\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#sam = SAM(nruns=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# SAM's output is a graph; we assume output_graph.edges() returns directed edges.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0moutput_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mest_adj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/cdt/causality/graph/SAM.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, data, graph, return_list_results)\u001b[0m\n\u001b[1;32m    515\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnruns\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m             results = [run_SAM(data, skeleton=skeleton,\n\u001b[0m\u001b[1;32m    518\u001b[0m                                \u001b[0mlr_gen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_disc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m                                \u001b[0mis_mixed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmixed_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/cdt/causality/graph/SAM.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    515\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnruns\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m             results = [run_SAM(data, skeleton=skeleton,\n\u001b[0m\u001b[1;32m    518\u001b[0m                                \u001b[0mlr_gen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_disc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m                                \u001b[0mis_mixed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmixed_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/cdt/causality/graph/SAM.py\u001b[0m in \u001b[0;36mrun_SAM\u001b[0;34m(in_data, skeleton, is_mixed, device, train, test, batch_size, lr_gen, lr_disc, lambda1, lambda2, nh, dnh, verbose, losstype, functionalComplexity, sampletype, dagstart, dagloss, dagpenalization, dagpenalization_increase, categorical_threshold, linear, numberHiddenLayersG, numberHiddenLayersD, idx)\u001b[0m\n\u001b[1;32m    304\u001b[0m                     \u001b[0mdisc_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisc_vars_d\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnb_var\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_vars_disc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m                 \u001b[0mdisc_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m                 \u001b[0md_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# ------------------------------------------------\n",
        "# 4. SAM Method (using the cdt Package)\n",
        "# ------------------------------------------------\n",
        "\n",
        "#import multiprocessing as mp\n",
        "#mp.set_start_method('spawn', force=True) # Before your SAM code\n",
        "import cdt\n",
        "\n",
        "def run_sam_method(X_data, nruns=1):\n",
        "    \"\"\"\n",
        "    Run the SAM algorithm from cdt.\n",
        "    Convert the simulated data (NumPy array) into a pandas DataFrame,\n",
        "    then run SAM. The output graph is converted to a binary adjacency matrix\n",
        "    where an edge i → j is denoted by a 1 at position [i, j].\n",
        "    \"\"\"\n",
        "    try:\n",
        "        from cdt.causality.graph import SAM\n",
        "        import cdt\n",
        "    except ImportError:\n",
        "        raise ImportError(\"cdt package is required for SAM. Install with 'pip install cdt'.\")\n",
        "\n",
        "    cdt.SETTINGS.GPU = 0 # use CPU\n",
        "    data_df = pd.DataFrame(X_data)\n",
        "    sam = SAM(nruns=nruns) #sam = SAM(nruns=1)\n",
        "    # SAM's output is a graph; we assume output_graph.edges() returns directed edges.\n",
        "    output_graph = sam.predict(data_df)\n",
        "    d = X_data.shape[1]\n",
        "    est_adj = np.zeros((d, d), dtype=int)\n",
        "    # For each edge in the graph, assume it is directed: if there is an edge from u to v,\n",
        "    # then set est_adj[u, v] = 1.\n",
        "    for (u, v) in output_graph.edges():\n",
        "        est_adj[u, v] = 1\n",
        "    # Optionally remove cycles.\n",
        "    G_nx = nx.DiGraph(est_adj)\n",
        "    remove_all_cycles(G_nx)\n",
        "    final_adj = nx.to_numpy_array(G_nx, dtype=int)\n",
        "    return final_adj\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 5. Main Comparison Loop: Testing Configurations\n",
        "# ------------------------------------------------\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    run_cell = True  # Set this to True to execute the comparison\n",
        "    if not run_cell:\n",
        "        class StopExecution(Exception):\n",
        "            def __init__(self, msg):\n",
        "                self.msg = msg\n",
        "            def _render_traceback_(self):\n",
        "                print(self.msg)\n",
        "                return []\n",
        "        raise StopExecution(\"Execution halted.\")\n",
        "\n",
        "    # Experimental configurations\n",
        "    var_settings = [100,500]      # Number of variables\n",
        "    dag_types = ['ER', 'SF']           # DAG types: Erdős–Rényi, Scale‑Free\n",
        "    sem_types = ['linear', 'non-linear']\n",
        "    noise_types = ['gaussian', 'exponential', 'laplace']  # Use \"laplace\" for Laplace noise\n",
        "    n_samples = 500                    # Samples per experiment\n",
        "    weight_scale = 1.0\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for n_vars in var_settings:\n",
        "        edge_prob = 2.0 / (n_vars - 1)  # Expected average degree ~2\n",
        "        for dag_type in dag_types:\n",
        "            # Generate true DAG\n",
        "            if dag_type == 'ER':\n",
        "                true_adj = generate_erdos_renyi_dag(n_vars, edge_prob)\n",
        "            elif dag_type == 'SF':\n",
        "                true_adj = generate_scale_free_dag(n_vars)\n",
        "            else:\n",
        "                continue\n",
        "            for sem_type in sem_types:\n",
        "                for noise in noise_types:\n",
        "                    config_str = f\"n_vars={n_vars}, DAG={dag_type}, SEM={sem_type}, Noise={noise}\"\n",
        "                    print(\"\\n[Config] \" + config_str)\n",
        "                    # Simulate data and get true weight matrix\n",
        "                    if sem_type == 'linear':\n",
        "                        X_data, W_true = simulate_sem_with_weights(true_adj, n_samples,\n",
        "                                                                   noise_type=noise,\n",
        "                                                                   weight_scale=weight_scale)\n",
        "                    else:\n",
        "                        X_data, W_true = simulate_non_linear_sem_with_weights(true_adj, n_samples,\n",
        "                                                                              noise_type=noise,\n",
        "                                                                              weight_scale=weight_scale,\n",
        "                                                                              non_linear_fn=np.tanh)\n",
        "                    # Create normalized data (standardization: zero mean, unit std)\n",
        "                    X_data_norm = (X_data - X_data.mean(axis=0)) / (X_data.std(axis=0) + 1e-8)\n",
        "                    # ---------------------------\n",
        "                    # Run SAM Method\n",
        "                    try:\n",
        "                        t0 = time.time()\n",
        "                        est_adj_sam = run_sam_method(X_data, nruns=1)\n",
        "                        runtime_sam = time.time() - t0\n",
        "                        shd_sam = shd(true_adj, est_adj_sam)\n",
        "                        sid_sam = sid(true_adj, est_adj_sam)\n",
        "                        ate_rmse_sam = rmse_ate(W_true, est_adj_sam.astype(float))\n",
        "                    except Exception as e:\n",
        "                        runtime_sam = np.nan\n",
        "                        shd_sam = np.nan\n",
        "                        sid_sam = np.nan\n",
        "                        ate_rmse_sam = np.nan\n",
        "                        print(\"  SAM method failed with error:\", e)\n",
        "\n",
        "                    results.append({\n",
        "                        'Method': 'SAM',\n",
        "                        'n_vars': n_vars,\n",
        "                        'DAG_type': dag_type,\n",
        "                        'SEM_type': sem_type,\n",
        "                        'Noise': noise,\n",
        "                        'Runtime_sec': runtime_sam,\n",
        "                        'SHD': shd_sam,\n",
        "                        'SID': sid_sam,\n",
        "                        'ATE_RMSE': ate_rmse_sam\n",
        "                    })\n",
        "\n",
        "                    # ---------------------------\n",
        "                    # Run DAG‑Boosting (FLAR) Method\n",
        "                    try:\n",
        "                        t0 = time.time()\n",
        "                        est_adj_dagboost, trainer = flar(X_data, threshold=0.3, verbose=False)\n",
        "                        runtime_dagboost = time.time() - t0\n",
        "                        shd_dagboost = shd(true_adj, est_adj_dagboost)\n",
        "                        sid_dagboost = sid(true_adj, est_adj_dagboost)\n",
        "                        ate_rmse_dagboost = rmse_ate(W_true, est_adj_dagboost.astype(float))\n",
        "                    except Exception as e:\n",
        "                        runtime_dagboost = np.nan\n",
        "                        shd_dagboost = np.nan\n",
        "                        sid_dagboost = np.nan\n",
        "                        ate_rmse_dagboost = np.nan\n",
        "                        print(\"  DAG‑Boosting method failed with error:\", e)\n",
        "\n",
        "                    results.append({\n",
        "                        'Method': 'DAGBoost',\n",
        "                        'n_vars': n_vars,\n",
        "                        'DAG_type': dag_type,\n",
        "                        'SEM_type': sem_type,\n",
        "                        'Noise': noise,\n",
        "                        'Runtime_sec': runtime_dagboost,\n",
        "                        'SHD': shd_dagboost,\n",
        "                        'SID': sid_dagboost,\n",
        "                        'ATE_RMSE': ate_rmse_dagboost\n",
        "                    })\n",
        "\n",
        "    df_results = pd.DataFrame(results)\n",
        "    print(\"\\n===== Comparison Results =====\")\n",
        "    print(df_results)\n",
        "\n",
        "    # Save the results DataFrame as CSV and JSON files\n",
        "    output_csv_filename = \"SAM_results.csv\"\n",
        "    output_json_filename = \"SAM_results.json\"\n",
        "\n",
        "    # Save to CSV (with headers, no index)\n",
        "    df_results.to_csv(output_csv_filename, index=False)\n",
        "    print(f\"Results saved to {output_csv_filename}\")\n",
        "\n",
        "    # Save to JSON in records format (one JSON object per line)\n",
        "    df_results.to_json(output_json_filename, orient=\"records\", lines=True)\n",
        "    print(f\"Results saved to {output_json_filename}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "008b517751b249a3beef3d74cdf2e274": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4add9933905245ce8d73d9a89f566ae9",
              "IPY_MODEL_30a04890926d46389e616baaaa09fa2e",
              "IPY_MODEL_954ad3ad2f6042e68a991345af5ad30a"
            ],
            "layout": "IPY_MODEL_349e14a5ca9e45c081293b22ca1ef84f"
          }
        },
        "07f7ab3a4b514832aa55a7c7000fd32b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "30a04890926d46389e616baaaa09fa2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac8b0a4a3c0047ed9af019cad8a06f3a",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_07f7ab3a4b514832aa55a7c7000fd32b",
            "value": 100
          }
        },
        "349e14a5ca9e45c081293b22ca1ef84f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a87647b37144ad0954424a2dd782476": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4add9933905245ce8d73d9a89f566ae9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4a87647b37144ad0954424a2dd782476",
            "placeholder": "​",
            "style": "IPY_MODEL_acff3eace9e94ed8b170a510c901bc2e",
            "value": "Depth=4, working on node 99: 100%"
          }
        },
        "83d81d346c7f44b690d1463bb1fdd3be": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "954ad3ad2f6042e68a991345af5ad30a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e6b05f760a8b40ff9b8d5afdb1b02c17",
            "placeholder": "​",
            "style": "IPY_MODEL_83d81d346c7f44b690d1463bb1fdd3be",
            "value": " 100/100 [00:00&lt;00:00, 1803.33it/s]"
          }
        },
        "ac8b0a4a3c0047ed9af019cad8a06f3a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "acff3eace9e94ed8b170a510c901bc2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e6b05f760a8b40ff9b8d5afdb1b02c17": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
